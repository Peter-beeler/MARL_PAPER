2026-02-24 06:00:41.088595: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2026-02-24 06:00:41.088594: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2026-02-24 06:00:41.088595: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2026-02-24 06:00:41.088604: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1771930841.576097 3582080 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1771930841.576097 3582079 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1771930841.576090 3582081 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1771930841.576101 3582078 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1771930841.772656 3582080 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1771930841.772664 3582081 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1771930841.772666 3582079 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1771930841.772675 3582078 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1771930842.993874 3582078 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1771930842.993880 3582080 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1771930842.993879 3582081 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1771930842.993881 3582079 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1771930842.993899 3582080 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1771930842.993899 3582078 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1771930842.993904 3582080 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1771930842.993904 3582078 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1771930842.993903 3582081 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1771930842.993904 3582079 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1771930842.993907 3582080 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1771930842.993907 3582078 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1771930842.993907 3582081 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1771930842.993908 3582079 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1771930842.993910 3582081 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1771930842.993910 3582079 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2026-02-24 06:00:43.148010: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-02-24 06:00:43.148010: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-02-24 06:00:43.148012: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-02-24 06:00:43.148043: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Using Accelerator with 4 processes
Loading model: Qwen/Qwen3-4B-Instruct-2507
Using Accelerator with 4 processes
Using Accelerator with 4 processes
Using Accelerator with 4 processes
Helper functions: ['move_to', 'clean_at', 'eat_at', 'random_explore']
Max new tokens: 384 (thinking=256 + action=128)
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:17<00:35, 17.86s/it]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:17<00:35, 17.86s/it]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:17<00:35, 17.86s/it]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:17<00:35, 17.86s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:38<00:19, 19.33s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:38<00:19, 19.38s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:38<00:19, 19.38s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:38<00:19, 19.38s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:38<00:00, 10.72s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:38<00:00, 10.72s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:38<00:00, 10.73s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:38<00:00, 12.90s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:38<00:00, 12.90s/it]

Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:38<00:00, 12.90s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:38<00:00, 10.72s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:38<00:00, 12.90s/it]
Applying LoRA to modules: ['up_proj', 'o_proj', 'v_proj', 'gate_proj', 'down_proj', 'q_proj', 'k_proj']
Number of trainable parameters: 16515072
Number of trainable parameters: 16515072
trainable params: 16,515,072 || all params: 4,038,983,168 || trainable%: 0.4089
[CUDA Memory - After LoRA model loaded] Allocated: 0.00GB, Reserved: 0.00GB, Peak: 0.00GB
GRPO with inner epochs: Old model will be created on first group
Number of trainable parameters: 16515072
Number of trainable parameters: 16515072
NCCL version 2.21.5+cuda12.4
[GPU3] Model prepared
[GPU0] Model prepared
[GPU2] Model prepared
[GPU1] Model prepared

=== Pre-Training Evaluation ===

=== Pre-Training Evaluation ===

=== Pre-Training Evaluation ===
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: maoyi8219 (maoyi8219-ohio-state-buckeyes) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/wandb/run-20260224_060153-vf7tpgrn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run compound_run_clean0.2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/maoyi8219-ohio-state-buckeyes/grpo
wandb: üöÄ View run at https://wandb.ai/maoyi8219-ohio-state-buckeyes/grpo/runs/vf7tpgrn
Initialized wandb project: grpo
Generated 20 fixed evaluation states

=== Pre-Training Evaluation ===

=== Evaluation (10 episodes) ===
Moving from (7, 5) towards (9, 5).
Moving from (7, 4) towards (9, 5).
Moving from (9, 4) towards (10, 3).
Moving from (6, 4) towards (4, 3).
  GPU2 Ep3: R=7.80, Steps=30, Time=101.19s
  GPU3 Ep4: R=2.80, Steps=30, Time=102.34s
  GPU0 Ep1: R=2.20, Steps=30, Time=101.15s
  GPU1 Ep2: R=1.60, Steps=30, Time=105.87s
Moving from (6, 7) towards (4, 8).
Moving from (8, 8) towards (13, 8).
  GPU2 Ep7: R=3.00, Steps=30, Time=97.19s
Moving from (7, 5) towards (13, 5).
Moving from (9, 8) towards (13, 8).
  GPU3 Ep8: R=5.20, Steps=30, Time=98.21s
  GPU0 Ep5: R=6.20, Steps=30, Time=100.32s
  GPU1 Ep6: R=3.60, Steps=30, Time=102.69s
Moving from (7, 4) towards (5, 4).
Moving from (7, 5) towards (3, 5).
Moving from (6, 5) towards (3, 5).
  GPU0 Ep9: R=6.20, Steps=30, Time=99.31s
  GPU1 Ep10: R=5.60, Steps=30, Time=102.03s

Reward: 4.42¬±1.95 [1.60, 7.80]
Evaluation Time: avg=101.03s/episode, total=308.31s

======================================================================
GRPO Training with Compound High-Level Actions (Single-Stage)
======================================================================

[Model] Qwen/Qwen3-4B-Instruct-2507
[Loss Type] DRGRPO
[Environment] Move (directional actions via helpers)
[Helpers] ['move_to', 'clean_at', 'eat_at', 'random_explore']
[Max New Tokens] 384 (thinking=256 + action=128)
[Agents] 3
[Episodes] 2048
[Multi-GPU] 4 GPUs √ó 8 eps/GPU = 32 per group
======================================================================


[Group 0] Episodes 0-31 (expected)
Creating old model (first group)...
  ‚úì Old model created and frozen

  Sample generation (compound):
    Obs: You are at (5, 3). You see dirt at (7, 2) and dirt at (7, 4).
    Response (thinking+JSON): '{
    "action": "clean_at",
    "agent_id": 1,
    "args": {
        "coord_x": 7,
        "coord_y": 2
    }
}'
    ‚Üí Low-level action: right
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
  [GPU2] Ep0: R=3.00, Steps=30, Time=105.63s
  [GPU3] Ep0: R=3.00, Steps=30, Time=106.37s
[CUDA Memory - After episode rollout] Allocated: 30.43GB, Reserved: 43.29GB, Peak: 33.29GB
  [GPU1] Ep0: R=4.20, Steps=30, Time=109.54s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (3, 5) towards (3, 7).
Moving from (6, 3) towards (4, 2).
Moving from (6, 2) towards (4, 2).
  [GPU2] Ep1: R=3.00, Steps=30, Time=104.52s
Moving from (5, 3) towards (4, 2).
  [GPU3] Ep1: R=5.80, Steps=30, Time=105.99s
  [GPU0] Ep1: R=3.00, Steps=30, Time=107.32s
  [GPU1] Ep1: R=3.80, Steps=30, Time=109.21s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
  [GPU2] Ep2: R=3.00, Steps=30, Time=105.25s
  [GPU3] Ep2: R=3.00, Steps=30, Time=106.40s
  [GPU0] Ep2: R=4.00, Steps=30, Time=107.60s
  [GPU1] Ep2: R=2.80, Steps=30, Time=109.99s
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU2] Ep3: R=4.20, Steps=30, Time=105.05s
  [GPU3] Ep3: R=2.40, Steps=30, Time=106.24s
  [GPU0] Ep3: R=4.00, Steps=30, Time=107.45s
Moving from (6, 5) towards (2, 5).
Moving from (8, 8) towards (12, 8).
  [GPU1] Ep3: R=3.00, Steps=30, Time=109.79s
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU2] Ep4: R=3.00, Steps=30, Time=105.43s
  [GPU3] Ep4: R=2.80, Steps=30, Time=106.05s
  [GPU0] Ep4: R=3.00, Steps=30, Time=107.45s
  [GPU1] Ep4: R=4.20, Steps=30, Time=109.90s
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU2] Ep5: R=2.80, Steps=30, Time=105.42s
  [GPU3] Ep5: R=2.80, Steps=30, Time=106.35s
  [GPU0] Ep5: R=2.80, Steps=30, Time=107.03s
  [GPU1] Ep5: R=2.80, Steps=30, Time=109.62s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (2, 5) towards (1, 3).
Moving from (4, 6) towards (2, 5).
Moving from (1, 5) towards (0, 4).
  [GPU2] Ep6: R=4.00, Steps=30, Time=105.56s
Moving from (0, 4) towards (1, 3).
  [GPU3] Ep6: R=2.80, Steps=30, Time=106.07s
  [GPU0] Ep6: R=7.20, Steps=30, Time=107.29s
  [GPU1] Ep6: R=3.80, Steps=30, Time=109.95s
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (2, 5) towards (1, 3).
Moving from (4, 6) towards (2, 5).
Moving from (1, 5) towards (0, 4).
Moving from (6, 8) towards (1, 8).
Moving from (0, 4) towards (1, 3).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (2, 3) towards (2, 1).
  [GPU2] Ep7: R=7.20, Steps=30, Time=105.31s
  [GPU3] Ep7: R=3.80, Steps=30, Time=105.98s
  [GPU0] Ep7: R=4.00, Steps=30, Time=107.27s
  [GPU1] Ep7: R=3.80, Steps=30, Time=109.95s
  ‚úì Collected 32 episodes ([8, 8, 8, 8] per GPU)
  üìù Logged episode 6 trajectory
  Reward: 3.65¬±1.16 [2.40, 7.20], Steps: 30.0
  Low-level actions (GPU0 sample): up:17% | down:22% | left:22% | right:20% | clean:16% | eat:1% | stay:0%

  Inner Optimization (4 epochs, minibatch=8):
Training step failed: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 2.35 GiB is free. Including non-PyTorch memory, this process has 76.89 GiB memory in use. Of the allocated memory 74.26 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1243, in train
    loss, clip_fraction, n_samples = self.compute_loss_on_samples(micro_batch, model_device)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1012, in compute_loss_on_samples
    new_log_probs = self.compute_batch_sequence_log_prob(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 600, in compute_batch_sequence_log_prob
    logits = forward_pass()
             ^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 596, in forward_pass
    outputs = model(input_ids=batch_input_ids, attention_mask=attention_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 786, in convert_to_fp32
    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 118, in recursively_apply
    {
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 119, in <dictcomp>
    k: recursively_apply(
       ^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 778, in _convert_to_fp32
    return tensor.float()
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 2.35 GiB is free. Including non-PyTorch memory, this process has 76.89 GiB memory in use. Of the allocated memory 74.26 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[Group 1] Episodes 32-63 (expected)
Updating old model with current weights...
  ‚úì Old model updated
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
  [GPU2] Ep0: R=4.20, Steps=30, Time=106.58s
  [GPU3] Ep0: R=4.00, Steps=30, Time=107.21s
  [GPU0] Ep0: R=4.20, Steps=30, Time=108.70s
  [GPU1] Ep0: R=4.00, Steps=30, Time=110.59s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (3, 6) towards (2, 5).
Moving from (7, 7) towards (12, 8).
Moving from (8, 7) towards (12, 8).
  [GPU2] Ep1: R=3.20, Steps=30, Time=106.75s
  [GPU3] Ep1: R=3.00, Steps=30, Time=106.67s
  [GPU0] Ep1: R=3.80, Steps=30, Time=108.64s
  [GPU1] Ep1: R=4.20, Steps=30, Time=110.88s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU2] Ep2: R=4.20, Steps=30, Time=106.46s
  [GPU3] Ep2: R=4.00, Steps=30, Time=106.69s
Moving from (8, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
  [GPU0] Ep2: R=3.80, Steps=30, Time=108.00s
  [GPU1] Ep2: R=3.20, Steps=30, Time=110.95s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (7, 7) towards (12, 8).
Moving from (3, 5) towards (5, 5).
  [GPU3] Ep3: R=4.00, Steps=30, Time=106.62s
  [GPU2] Ep3: R=3.20, Steps=30, Time=107.63s
Moving from (8, 5) towards (5, 5).
Moving from (8, 6) towards (5, 6).
  [GPU0] Ep3: R=6.00, Steps=30, Time=107.33s
  [GPU1] Ep3: R=3.00, Steps=30, Time=109.40s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (2, 5) towards (1, 3).
Moving from (4, 6) towards (2, 5).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (0, 4) towards (1, 3).
Moving from (3, 5) towards (5, 5).
Moving from (7, 7) towards (12, 8).
Moving from (8, 5) towards (5, 5).
Moving from (8, 6) towards (5, 6).
  [GPU3] Ep4: R=7.20, Steps=30, Time=105.95s
  [GPU2] Ep4: R=6.00, Steps=30, Time=107.71s
  [GPU0] Ep4: R=3.20, Steps=30, Time=107.98s
  [GPU1] Ep4: R=4.00, Steps=30, Time=110.55s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (8, 5) towards (9, 6).
Moving from (7, 2) towards (4, 2).
  [GPU3] Ep5: R=3.80, Steps=30, Time=106.22s
Moving from (1, 5) towards (3, 7).
Moving from (3, 5) towards (5, 5).
Moving from (2, 5) towards (3, 7).
  [GPU2] Ep5: R=7.00, Steps=30, Time=108.22s
Moving from (8, 5) towards (5, 5).
Moving from (8, 6) towards (5, 6).
  [GPU0] Ep5: R=6.00, Steps=30, Time=107.79s
  [GPU1] Ep5: R=3.80, Steps=30, Time=110.76s
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (3, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU3] Ep6: R=4.20, Steps=30, Time=106.53s
  [GPU2] Ep6: R=3.80, Steps=30, Time=107.80s
  [GPU0] Ep6: R=3.00, Steps=30, Time=106.88s
  [GPU1] Ep6: R=3.80, Steps=30, Time=110.73s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (2, 7) towards (2, 5).
Moving from (2, 8) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (2, 7) towards (2, 5).
  [GPU3] Ep7: R=3.00, Steps=30, Time=106.79s
Moving from (6, 3) towards (9, 2).
Moving from (6, 2) towards (9, 2).
Moving from (1, 5) towards (3, 7).
Moving from (7, 2) towards (9, 2).
Moving from (2, 5) towards (3, 7).
  [GPU2] Ep7: R=6.00, Steps=30, Time=107.98s
Moving from (6, 3) towards (9, 2).
Moving from (6, 2) towards (9, 2).
Moving from (1, 5) towards (3, 7).
Moving from (7, 2) towards (9, 2).
Moving from (2, 5) towards (3, 7).
  [GPU0] Ep7: R=6.00, Steps=30, Time=108.71s
  [GPU1] Ep7: R=4.00, Steps=30, Time=110.95s
  ‚úì Collected 32 episodes ([8, 8, 8, 8] per GPU)
  üìù Logged episode 2 trajectory
  Reward: 4.28¬±1.19 [3.00, 7.20], Steps: 30.0
  Low-level actions (GPU0 sample): up:16% | down:21% | left:23% | right:20% | clean:16% | eat:1% | stay:0%

  Inner Optimization (4 epochs, minibatch=8):
Training step failed: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 2.14 GiB is free. Including non-PyTorch memory, this process has 77.10 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1243, in train
    loss, clip_fraction, n_samples = self.compute_loss_on_samples(micro_batch, model_device)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1012, in compute_loss_on_samples
    new_log_probs = self.compute_batch_sequence_log_prob(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 600, in compute_batch_sequence_log_prob
    logits = forward_pass()
             ^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 596, in forward_pass
    outputs = model(input_ids=batch_input_ids, attention_mask=attention_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 786, in convert_to_fp32
    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 118, in recursively_apply
    {
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 119, in <dictcomp>
    k: recursively_apply(
       ^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 778, in _convert_to_fp32
    return tensor.float()
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 2.14 GiB is free. Including non-PyTorch memory, this process has 77.10 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[Group 2] Episodes 64-95 (expected)
Updating old model with current weights...
  ‚úì Old model updated
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (3, 6) towards (2, 5).
  [GPU2] Ep0: R=3.00, Steps=30, Time=107.04s
  [GPU0] Ep0: R=3.00, Steps=30, Time=107.63s
  [GPU3] Ep0: R=2.80, Steps=30, Time=108.38s
  [GPU1] Ep0: R=4.20, Steps=30, Time=110.85s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU2] Ep1: R=4.00, Steps=30, Time=107.28s
  [GPU3] Ep1: R=4.00, Steps=30, Time=107.57s
  [GPU0] Ep1: R=4.00, Steps=30, Time=108.71s
  [GPU1] Ep1: R=3.00, Steps=30, Time=111.16s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (2, 5) towards (1, 3).
Moving from (1, 5) towards (0, 4).
Moving from (0, 4) towards (1, 3).
  [GPU2] Ep2: R=4.00, Steps=30, Time=107.79s
  [GPU3] Ep2: R=2.80, Steps=30, Time=107.51s
  [GPU0] Ep2: R=3.80, Steps=30, Time=108.56s
  [GPU1] Ep2: R=7.20, Steps=30, Time=110.91s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (3, 6) towards (2, 5).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (8, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (3, 5) towards (3, 7).
Moving from (6, 3) towards (4, 2).
Moving from (6, 2) towards (4, 2).
  [GPU2] Ep3: R=3.80, Steps=30, Time=108.00s
Moving from (5, 3) towards (4, 2).
  [GPU3] Ep3: R=5.80, Steps=30, Time=107.76s
  [GPU0] Ep3: R=3.20, Steps=30, Time=108.88s
  [GPU1] Ep3: R=2.80, Steps=30, Time=111.26s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
  [GPU2] Ep4: R=4.00, Steps=30, Time=108.24s
  [GPU3] Ep4: R=3.80, Steps=30, Time=107.75s
  [GPU0] Ep4: R=2.80, Steps=30, Time=108.65s
  [GPU1] Ep4: R=2.40, Steps=30, Time=111.53s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU3] Ep5: R=2.80, Steps=30, Time=107.14s
  [GPU2] Ep5: R=3.80, Steps=30, Time=107.93s
  [GPU0] Ep5: R=2.40, Steps=30, Time=109.21s
  [GPU1] Ep5: R=3.80, Steps=30, Time=111.29s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU3] Ep6: R=4.00, Steps=30, Time=107.71s
  [GPU2] Ep6: R=2.80, Steps=30, Time=107.59s
  [GPU0] Ep6: R=4.00, Steps=30, Time=108.91s
  [GPU1] Ep6: R=2.80, Steps=30, Time=110.71s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (8, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (8, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
  [GPU2] Ep7: R=4.00, Steps=30, Time=107.74s
  [GPU3] Ep7: R=3.20, Steps=30, Time=108.06s
  [GPU0] Ep7: R=3.20, Steps=30, Time=109.37s
  [GPU1] Ep7: R=4.00, Steps=30, Time=111.24s
  ‚úì Collected 32 episodes ([8, 8, 8, 8] per GPU)
  üìù Logged episode 1 trajectory
  Reward: 3.60¬±0.96 [2.40, 7.20], Steps: 30.0
  Low-level actions (GPU0 sample): up:18% | down:21% | left:22% | right:20% | clean:16% | eat:0% | stay:0%

  Inner Optimization (4 epochs, minibatch=8):
Training step failed: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.77 GiB is free. Including non-PyTorch memory, this process has 77.47 GiB memory in use. Of the allocated memory 74.21 GiB is allocated by PyTorch, and 1.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1243, in train
    loss, clip_fraction, n_samples = self.compute_loss_on_samples(micro_batch, model_device)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1012, in compute_loss_on_samples
    new_log_probs = self.compute_batch_sequence_log_prob(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 600, in compute_batch_sequence_log_prob
    logits = forward_pass()
             ^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 596, in forward_pass
    outputs = model(input_ids=batch_input_ids, attention_mask=attention_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 786, in convert_to_fp32
    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 118, in recursively_apply
    {
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 119, in <dictcomp>
    k: recursively_apply(
       ^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 778, in _convert_to_fp32
    return tensor.float()
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.77 GiB is free. Including non-PyTorch memory, this process has 77.47 GiB memory in use. Of the allocated memory 74.21 GiB is allocated by PyTorch, and 1.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[Group 3] Episodes 96-127 (expected)
Updating old model with current weights...
  ‚úì Old model updated
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (4, 6) towards (2, 5).
  [GPU2] Ep0: R=2.80, Steps=30, Time=106.10s
  [GPU3] Ep0: R=4.00, Steps=30, Time=107.78s
  [GPU0] Ep0: R=3.80, Steps=30, Time=109.20s
  [GPU1] Ep0: R=4.00, Steps=30, Time=111.15s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (3, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU2] Ep1: R=4.20, Steps=30, Time=107.74s
  [GPU3] Ep1: R=4.00, Steps=30, Time=107.80s
  [GPU0] Ep1: R=4.20, Steps=30, Time=109.55s
  [GPU1] Ep1: R=3.80, Steps=30, Time=111.26s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (3, 5) towards (3, 7).
Moving from (6, 3) towards (4, 2).
Moving from (6, 2) towards (4, 2).
Moving from (5, 3) towards (4, 2).
  [GPU2] Ep2: R=5.80, Steps=30, Time=107.42s
  [GPU3] Ep2: R=3.80, Steps=30, Time=107.32s
Moving from (3, 5) towards (3, 7).
  [GPU0] Ep2: R=4.20, Steps=30, Time=109.41s
Moving from (6, 3) towards (4, 2).
Moving from (6, 2) towards (4, 2).
Moving from (5, 3) towards (4, 2).
  [GPU1] Ep2: R=5.80, Steps=30, Time=111.50s
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (3, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (3, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (3, 8) towards (1, 8).
Moving from (2, 7) towards (2, 5).
Moving from (8, 5) towards (11, 5).
Moving from (8, 2) towards (11, 2).
  [GPU2] Ep3: R=6.00, Steps=30, Time=106.81s
  [GPU3] Ep3: R=4.20, Steps=30, Time=107.39s
  [GPU0] Ep3: R=4.00, Steps=30, Time=110.22s
  [GPU1] Ep3: R=4.00, Steps=30, Time=111.53s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (3, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
  [GPU2] Ep4: R=4.20, Steps=30, Time=106.21s
Moving from (8, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
  [GPU3] Ep4: R=2.40, Steps=30, Time=107.74s
  [GPU0] Ep4: R=3.00, Steps=30, Time=109.31s
  [GPU1] Ep4: R=3.80, Steps=30, Time=110.93s
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU2] Ep5: R=4.20, Steps=30, Time=106.40s
  [GPU3] Ep5: R=4.00, Steps=30, Time=107.83s
  [GPU0] Ep5: R=3.00, Steps=30, Time=109.09s
  [GPU1] Ep5: R=4.20, Steps=30, Time=110.88s
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
  [GPU2] Ep6: R=3.00, Steps=30, Time=106.36s
  [GPU3] Ep6: R=4.00, Steps=30, Time=107.35s
  [GPU0] Ep6: R=3.00, Steps=30, Time=108.72s
  [GPU1] Ep6: R=4.20, Steps=30, Time=111.01s
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
  [GPU2] Ep7: R=3.00, Steps=30, Time=106.33s
  [GPU3] Ep7: R=3.80, Steps=30, Time=107.68s
  [GPU0] Ep7: R=4.20, Steps=30, Time=109.12s
  [GPU1] Ep7: R=4.20, Steps=30, Time=112.15s
  ‚úì Collected 32 episodes ([8, 8, 8, 8] per GPU)
  üìù Logged episode 7 trajectory
  Reward: 3.96¬±0.80 [2.40, 6.00], Steps: 30.0
  Low-level actions (GPU0 sample): up:16% | down:21% | left:22% | right:20% | clean:16% | eat:0% | stay:0%

  Inner Optimization (4 epochs, minibatch=8):
Training step failed: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 2.15 GiB is free. Including non-PyTorch memory, this process has 77.09 GiB memory in use. Of the allocated memory 74.21 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1243, in train
    loss, clip_fraction, n_samples = self.compute_loss_on_samples(micro_batch, model_device)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1012, in compute_loss_on_samples
    new_log_probs = self.compute_batch_sequence_log_prob(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 600, in compute_batch_sequence_log_prob
    logits = forward_pass()
             ^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 596, in forward_pass
    outputs = model(input_ids=batch_input_ids, attention_mask=attention_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 786, in convert_to_fp32
    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 118, in recursively_apply
    {
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 119, in <dictcomp>
    k: recursively_apply(
       ^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 778, in _convert_to_fp32
    return tensor.float()
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 2.15 GiB is free. Including non-PyTorch memory, this process has 77.09 GiB memory in use. Of the allocated memory 74.21 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[Group 4] Episodes 128-159 (expected)
Updating old model with current weights...
  ‚úì Old model updated
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 8) towards (2, 5).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (7, 3) towards (5, 2).
Moving from (7, 3) towards (5, 2).
Moving from (7, 3) towards (5, 2).
Moving from (6, 3) towards (5, 2).
Moving from (3, 5) towards (5, 5).
Moving from (6, 3) towards (5, 2).
Moving from (3, 5) towards (5, 5).
Moving from (6, 3) towards (5, 2).
Moving from (3, 5) towards (5, 5).
  [GPU2] Ep0: R=7.00, Steps=30, Time=106.35s
  [GPU3] Ep0: R=7.00, Steps=30, Time=107.61s
  [GPU0] Ep0: R=7.00, Steps=30, Time=108.99s
  [GPU1] Ep0: R=3.00, Steps=30, Time=111.58s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU2] Ep1: R=4.20, Steps=30, Time=106.30s
  [GPU3] Ep1: R=4.20, Steps=30, Time=107.21s
  [GPU0] Ep1: R=3.00, Steps=30, Time=109.34s
  [GPU1] Ep1: R=3.80, Steps=30, Time=110.58s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU2] Ep2: R=2.80, Steps=30, Time=106.11s
  [GPU3] Ep2: R=2.80, Steps=30, Time=106.16s
  [GPU0] Ep2: R=2.40, Steps=30, Time=109.22s
  [GPU1] Ep2: R=4.20, Steps=30, Time=112.13s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU2] Ep3: R=3.00, Steps=30, Time=106.38s
  [GPU3] Ep3: R=4.20, Steps=30, Time=106.46s
  [GPU0] Ep3: R=2.80, Steps=30, Time=108.62s
  [GPU1] Ep3: R=2.80, Steps=30, Time=111.12s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
  [GPU2] Ep4: R=2.80, Steps=30, Time=106.45s
  [GPU3] Ep4: R=3.00, Steps=30, Time=106.67s
Moving from (5, 5) towards (5, 3).
  [GPU0] Ep4: R=3.80, Steps=30, Time=109.20s
  [GPU1] Ep4: R=6.20, Steps=30, Time=112.23s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU2] Ep5: R=3.00, Steps=30, Time=106.57s
  [GPU3] Ep5: R=2.40, Steps=30, Time=107.19s
  [GPU0] Ep5: R=4.00, Steps=30, Time=109.36s
  [GPU1] Ep5: R=4.00, Steps=30, Time=111.84s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU2] Ep6: R=3.80, Steps=30, Time=106.40s
  [GPU3] Ep6: R=2.80, Steps=30, Time=107.07s
  [GPU0] Ep6: R=3.00, Steps=30, Time=109.27s
  [GPU1] Ep6: R=3.00, Steps=30, Time=112.37s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (4, 6) towards (2, 5).
  [GPU2] Ep7: R=2.40, Steps=30, Time=106.62s
Moving from (2, 7) towards (2, 5).
  [GPU3] Ep7: R=4.00, Steps=30, Time=107.65s
Moving from (2, 5) towards (3, 6).
Moving from (3, 6) towards (4, 7).
  [GPU0] Ep7: R=5.80, Steps=30, Time=109.28s
  [GPU1] Ep7: R=3.80, Steps=30, Time=112.14s
  ‚úì Collected 32 episodes ([8, 8, 8, 8] per GPU)
  üìù Logged episode 1 trajectory
  Reward: 3.81¬±1.36 [2.40, 7.00], Steps: 30.0
  Low-level actions (GPU0 sample): up:19% | down:24% | left:20% | right:18% | clean:15% | eat:1% | stay:0%

  Inner Optimization (4 epochs, minibatch=8):
Training step failed: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 2.07 GiB is free. Including non-PyTorch memory, this process has 77.17 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1243, in train
    loss, clip_fraction, n_samples = self.compute_loss_on_samples(micro_batch, model_device)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1012, in compute_loss_on_samples
    new_log_probs = self.compute_batch_sequence_log_prob(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 600, in compute_batch_sequence_log_prob
    logits = forward_pass()
             ^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 596, in forward_pass
    outputs = model(input_ids=batch_input_ids, attention_mask=attention_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 786, in convert_to_fp32
    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 118, in recursively_apply
    {
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 119, in <dictcomp>
    k: recursively_apply(
       ^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 778, in _convert_to_fp32
    return tensor.float()
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 2.07 GiB is free. Including non-PyTorch memory, this process has 77.17 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[Group 5] Episodes 160-191 (expected)
Updating old model with current weights...
  ‚úì Old model updated

  Sample generation (compound):
    Obs: You are at (5, 3). You see dirt at (7, 2) and dirt at (7, 4).
    Response (thinking+JSON): '{
    "action": "clean_at",
    "agent_id": 1,
    "args": {
        "coord_x": 7,
        "coord_y": 2
    }
}'
    ‚Üí Low-level action: right
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 6) towards (2, 5).
  [GPU3] Ep0: R=3.00, Steps=30, Time=107.17s
  [GPU2] Ep0: R=4.20, Steps=30, Time=107.21s
[CUDA Memory - After episode rollout] Allocated: 30.43GB, Reserved: 76.69GB, Peak: 74.33GB
  [GPU1] Ep0: R=4.00, Steps=30, Time=112.37s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU3] Ep1: R=4.00, Steps=30, Time=107.00s
  [GPU2] Ep1: R=3.00, Steps=30, Time=107.51s
  [GPU0] Ep1: R=4.00, Steps=30, Time=109.20s
  [GPU1] Ep1: R=4.00, Steps=30, Time=111.58s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
  [GPU3] Ep2: R=3.80, Steps=30, Time=106.05s
  [GPU2] Ep2: R=2.40, Steps=30, Time=106.79s
  [GPU0] Ep2: R=4.20, Steps=30, Time=108.86s
  [GPU1] Ep2: R=3.00, Steps=30, Time=111.55s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
  [GPU3] Ep3: R=3.20, Steps=30, Time=106.30s
  [GPU2] Ep3: R=3.80, Steps=30, Time=106.25s
  [GPU0] Ep3: R=3.60, Steps=30, Time=108.47s
  [GPU1] Ep3: R=3.00, Steps=30, Time=110.66s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
  [GPU3] Ep4: R=3.80, Steps=30, Time=106.45s
  [GPU2] Ep4: R=4.00, Steps=30, Time=106.15s
Moving from (3, 5) towards (3, 7).
Moving from (6, 3) towards (4, 2).
Moving from (6, 2) towards (4, 2).
Moving from (5, 3) towards (4, 2).
  [GPU0] Ep4: R=5.80, Steps=30, Time=108.94s
  [GPU1] Ep4: R=4.00, Steps=30, Time=112.20s
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (8, 8) towards (12, 8).
  [GPU2] Ep5: R=3.80, Steps=30, Time=105.67s
  [GPU3] Ep5: R=3.00, Steps=30, Time=107.37s
  [GPU0] Ep5: R=4.20, Steps=30, Time=109.00s
  [GPU1] Ep5: R=4.00, Steps=30, Time=111.88s
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU2] Ep6: R=2.80, Steps=30, Time=105.46s
  [GPU3] Ep6: R=2.40, Steps=30, Time=107.71s
  [GPU0] Ep6: R=3.20, Steps=30, Time=109.25s
  [GPU1] Ep6: R=2.40, Steps=30, Time=111.49s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU2] Ep7: R=4.00, Steps=30, Time=105.65s
  [GPU3] Ep7: R=4.00, Steps=30, Time=107.99s
  [GPU0] Ep7: R=3.00, Steps=30, Time=109.30s
  [GPU1] Ep7: R=3.00, Steps=30, Time=111.97s
  ‚úì Collected 32 episodes ([8, 8, 8, 8] per GPU)
  üìù Logged episode 2 trajectory
  Reward: 3.53¬±0.73 [2.40, 5.80], Steps: 30.0
  Low-level actions (GPU0 sample): up:17% | down:24% | left:21% | right:18% | clean:16% | eat:0% | stay:0%

  Inner Optimization (4 epochs, minibatch=8):
Training step failed: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.99 GiB is free. Including non-PyTorch memory, this process has 77.25 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1243, in train
    loss, clip_fraction, n_samples = self.compute_loss_on_samples(micro_batch, model_device)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1012, in compute_loss_on_samples
    new_log_probs = self.compute_batch_sequence_log_prob(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 600, in compute_batch_sequence_log_prob
    logits = forward_pass()
             ^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 596, in forward_pass
    outputs = model(input_ids=batch_input_ids, attention_mask=attention_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 786, in convert_to_fp32
    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 118, in recursively_apply
    {
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 119, in <dictcomp>
    k: recursively_apply(
       ^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 778, in _convert_to_fp32
    return tensor.float()
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.99 GiB is free. Including non-PyTorch memory, this process has 77.25 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[Group 6] Episodes 192-223 (expected)
Updating old model with current weights...
  ‚úì Old model updated
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (8, 7) towards (12, 8).
Moving from (3, 5) towards (3, 7).
Moving from (9, 7) towards (12, 8).
Moving from (6, 3) towards (4, 2).
Moving from (6, 2) towards (4, 2).
Moving from (10, 7) towards (12, 8).
Moving from (5, 3) towards (4, 2).
  [GPU2] Ep0: R=5.80, Steps=30, Time=106.55s
  [GPU3] Ep0: R=4.20, Steps=30, Time=106.70s
  [GPU0] Ep0: R=3.00, Steps=30, Time=109.00s
  [GPU1] Ep0: R=3.00, Steps=30, Time=112.69s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
  [GPU2] Ep1: R=3.00, Steps=30, Time=106.51s
  [GPU3] Ep1: R=4.00, Steps=30, Time=106.45s
  [GPU0] Ep1: R=2.80, Steps=30, Time=109.04s
  [GPU1] Ep1: R=4.20, Steps=30, Time=111.54s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
  [GPU2] Ep2: R=4.00, Steps=30, Time=106.42s
  [GPU3] Ep2: R=2.80, Steps=30, Time=106.48s
Moving from (3, 5) towards (3, 7).
Moving from (6, 3) towards (4, 2).
Moving from (6, 2) towards (4, 2).
Moving from (5, 3) towards (4, 2).
  [GPU0] Ep2: R=5.80, Steps=30, Time=109.17s
  [GPU1] Ep2: R=3.00, Steps=30, Time=112.14s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
  [GPU3] Ep3: R=2.80, Steps=30, Time=105.91s
  [GPU2] Ep3: R=4.00, Steps=30, Time=106.31s
  [GPU0] Ep3: R=3.80, Steps=30, Time=108.99s
  [GPU1] Ep3: R=2.80, Steps=30, Time=111.78s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
  [GPU2] Ep4: R=3.60, Steps=30, Time=106.42s
  [GPU3] Ep4: R=3.00, Steps=30, Time=106.68s
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (3, 5) towards (5, 5).
Moving from (8, 5) towards (5, 5).
Moving from (8, 6) towards (5, 6).
  [GPU0] Ep4: R=6.00, Steps=30, Time=109.14s
Moving from (8, 5) towards (5, 5).
Moving from (8, 6) towards (5, 6).
  [GPU1] Ep4: R=6.00, Steps=30, Time=111.39s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
  [GPU2] Ep5: R=2.80, Steps=30, Time=106.16s
  [GPU3] Ep5: R=3.00, Steps=30, Time=106.65s
  [GPU0] Ep5: R=4.00, Steps=30, Time=109.27s
  [GPU1] Ep5: R=4.20, Steps=30, Time=112.33s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (8, 6) towards (12, 8).
Moving from (8, 7) towards (12, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
  [GPU2] Ep6: R=3.20, Steps=30, Time=106.52s
  [GPU3] Ep6: R=4.20, Steps=30, Time=106.90s
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
  [GPU0] Ep6: R=3.80, Steps=30, Time=109.26s
Moving from (7, 3) towards (5, 2).
Moving from (6, 3) towards (5, 2).
  [GPU1] Ep6: R=7.00, Steps=30, Time=111.56s
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU2] Ep7: R=3.00, Steps=30, Time=106.34s
  [GPU3] Ep7: R=3.80, Steps=30, Time=106.19s
  [GPU0] Ep7: R=3.00, Steps=30, Time=108.42s
  [GPU1] Ep7: R=2.80, Steps=30, Time=111.06s
  ‚úì Collected 32 episodes ([8, 8, 8, 8] per GPU)
  üìù Logged episode 4 trajectory
  Reward: 3.82¬±1.14 [2.80, 7.00], Steps: 30.0
  Low-level actions (GPU0 sample): up:17% | down:21% | left:23% | right:19% | clean:16% | eat:1% | stay:0%

  Inner Optimization (4 epochs, minibatch=8):
Training step failed: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.89 GiB is free. Including non-PyTorch memory, this process has 77.35 GiB memory in use. Of the allocated memory 74.21 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1243, in train
    loss, clip_fraction, n_samples = self.compute_loss_on_samples(micro_batch, model_device)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1012, in compute_loss_on_samples
    new_log_probs = self.compute_batch_sequence_log_prob(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 600, in compute_batch_sequence_log_prob
    logits = forward_pass()
             ^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 596, in forward_pass
    outputs = model(input_ids=batch_input_ids, attention_mask=attention_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 786, in convert_to_fp32
    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 118, in recursively_apply
    {
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 119, in <dictcomp>
    k: recursively_apply(
       ^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 778, in _convert_to_fp32
    return tensor.float()
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.89 GiB is free. Including non-PyTorch memory, this process has 77.35 GiB memory in use. Of the allocated memory 74.21 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[Group 7] Episodes 224-255 (expected)
Updating old model with current weights...
  ‚úì Old model updated
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (8, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (3, 5) towards (3, 7).
  [GPU3] Ep0: R=3.80, Steps=30, Time=106.78s
  [GPU2] Ep0: R=3.20, Steps=30, Time=107.03s
Moving from (6, 3) towards (4, 2).
Moving from (6, 2) towards (4, 2).
  [GPU0] Ep0: R=4.20, Steps=30, Time=110.06s
Moving from (5, 3) towards (4, 2).
  [GPU1] Ep0: R=5.80, Steps=30, Time=112.04s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (3, 6) towards (2, 5).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (8, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (8, 5) towards (5, 5).
Moving from (8, 6) towards (5, 6).
  [GPU3] Ep1: R=6.00, Steps=30, Time=106.25s
  [GPU2] Ep1: R=4.20, Steps=30, Time=106.63s
  [GPU0] Ep1: R=3.20, Steps=30, Time=109.95s
  [GPU1] Ep1: R=3.80, Steps=30, Time=111.36s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU3] Ep2: R=4.20, Steps=30, Time=106.65s
  [GPU2] Ep2: R=4.00, Steps=30, Time=106.39s
Moving from (7, 4) towards (5, 3).
  [GPU0] Ep2: R=5.20, Steps=30, Time=109.56s
  [GPU1] Ep2: R=3.80, Steps=30, Time=111.27s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
  [GPU3] Ep3: R=4.00, Steps=30, Time=106.41s
  [GPU2] Ep3: R=4.00, Steps=30, Time=106.26s
Moving from (6, 5) towards (2, 5).
Moving from (8, 8) towards (12, 8).
  [GPU0] Ep3: R=2.40, Steps=30, Time=109.78s
  [GPU1] Ep3: R=3.00, Steps=30, Time=111.65s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
  [GPU3] Ep4: R=4.00, Steps=30, Time=106.33s
  [GPU2] Ep4: R=4.00, Steps=30, Time=106.74s
  [GPU0] Ep4: R=4.00, Steps=30, Time=109.95s
  [GPU1] Ep4: R=3.00, Steps=30, Time=111.44s
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU3] Ep5: R=4.00, Steps=30, Time=106.56s
  [GPU2] Ep5: R=3.80, Steps=30, Time=106.43s
  [GPU0] Ep5: R=4.00, Steps=30, Time=109.54s
  [GPU1] Ep5: R=3.80, Steps=30, Time=111.14s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (8, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
  [GPU3] Ep6: R=3.80, Steps=30, Time=106.51s
  [GPU2] Ep6: R=3.00, Steps=30, Time=106.68s
  [GPU0] Ep6: R=2.80, Steps=30, Time=109.50s
  [GPU1] Ep6: R=2.40, Steps=30, Time=112.45s
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU3] Ep7: R=3.00, Steps=30, Time=106.60s
  [GPU2] Ep7: R=3.80, Steps=30, Time=106.32s
  [GPU0] Ep7: R=3.00, Steps=30, Time=108.59s
  [GPU1] Ep7: R=4.00, Steps=30, Time=112.32s
  ‚úì Collected 32 episodes ([8, 8, 8, 8] per GPU)
  üìù Logged episode 1 trajectory
  Reward: 3.79¬±0.81 [2.40, 6.00], Steps: 30.0
  Low-level actions (GPU0 sample): up:17% | down:22% | left:22% | right:20% | clean:16% | eat:0% | stay:0%

  Inner Optimization (4 epochs, minibatch=8):
Training step failed: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1243, in train
    loss, clip_fraction, n_samples = self.compute_loss_on_samples(micro_batch, model_device)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1012, in compute_loss_on_samples
    new_log_probs = self.compute_batch_sequence_log_prob(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 600, in compute_batch_sequence_log_prob
    logits = forward_pass()
             ^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 596, in forward_pass
    outputs = model(input_ids=batch_input_ids, attention_mask=attention_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 786, in convert_to_fp32
    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 118, in recursively_apply
    {
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 119, in <dictcomp>
    k: recursively_apply(
       ^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 778, in _convert_to_fp32
    return tensor.float()
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[Group 8] Episodes 256-287 (expected)
Updating old model with current weights...
  ‚úì Old model updated
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (8, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (8, 5) towards (9, 6).
Moving from (7, 2) towards (4, 2).
Moving from (1, 5) towards (3, 7).
  [GPU2] Ep0: R=3.00, Steps=30, Time=106.54s
  [GPU3] Ep0: R=3.20, Steps=30, Time=107.20s
Moving from (2, 5) towards (3, 7).
  [GPU0] Ep0: R=7.00, Steps=30, Time=110.20s
  [GPU1] Ep0: R=3.80, Steps=30, Time=112.41s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU2] Ep1: R=4.00, Steps=30, Time=106.30s
  [GPU3] Ep1: R=2.40, Steps=30, Time=106.80s
  [GPU0] Ep1: R=3.80, Steps=30, Time=109.18s
  [GPU1] Ep1: R=4.00, Steps=30, Time=111.74s
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (3, 5) towards (5, 5).
Moving from (8, 5) towards (5, 5).
Moving from (8, 6) towards (5, 6).
  [GPU2] Ep2: R=6.00, Steps=30, Time=106.18s
  [GPU3] Ep2: R=4.00, Steps=30, Time=106.95s
  [GPU0] Ep2: R=2.80, Steps=30, Time=108.84s
  [GPU1] Ep2: R=2.80, Steps=30, Time=111.71s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (3, 6) towards (2, 5).
  [GPU2] Ep3: R=4.00, Steps=30, Time=106.07s
  [GPU3] Ep3: R=4.00, Steps=30, Time=106.30s
  [GPU0] Ep3: R=3.80, Steps=30, Time=109.09s
  [GPU1] Ep3: R=3.80, Steps=30, Time=111.83s
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU2] Ep4: R=3.80, Steps=30, Time=106.25s
  [GPU3] Ep4: R=3.80, Steps=30, Time=106.26s
  [GPU0] Ep4: R=3.80, Steps=30, Time=109.54s
  [GPU1] Ep4: R=2.40, Steps=30, Time=112.97s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
  [GPU2] Ep5: R=4.20, Steps=30, Time=106.05s
  [GPU3] Ep5: R=3.80, Steps=30, Time=106.42s
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (2, 6) towards (3, 7).
  [GPU0] Ep5: R=5.80, Steps=30, Time=109.61s
Moving from (6, 3) towards (9, 2).
Moving from (6, 2) towards (9, 2).
Moving from (1, 5) towards (3, 7).
Moving from (7, 2) towards (9, 2).
Moving from (2, 5) towards (3, 7).
  [GPU1] Ep5: R=6.00, Steps=30, Time=111.73s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (3, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU2] Ep6: R=2.40, Steps=30, Time=106.47s
  [GPU3] Ep6: R=3.00, Steps=30, Time=105.29s
  [GPU0] Ep6: R=3.80, Steps=30, Time=109.37s
  [GPU1] Ep6: R=2.40, Steps=30, Time=112.25s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU2] Ep7: R=3.20, Steps=30, Time=106.47s
  [GPU3] Ep7: R=3.00, Steps=30, Time=105.57s
  [GPU0] Ep7: R=3.80, Steps=30, Time=109.33s
  [GPU1] Ep7: R=2.80, Steps=30, Time=112.20s
  ‚úì Collected 32 episodes ([8, 8, 8, 8] per GPU)
  üìù Logged episode 3 trajectory
  Reward: 3.76¬±1.10 [2.40, 7.00], Steps: 30.0
  Low-level actions (GPU0 sample): up:17% | down:20% | left:22% | right:20% | clean:15% | eat:1% | stay:0%

  Inner Optimization (4 epochs, minibatch=8):
Training step failed: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 2.33 GiB is free. Including non-PyTorch memory, this process has 76.91 GiB memory in use. Of the allocated memory 74.21 GiB is allocated by PyTorch, and 1.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1243, in train
    loss, clip_fraction, n_samples = self.compute_loss_on_samples(micro_batch, model_device)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1012, in compute_loss_on_samples
    new_log_probs = self.compute_batch_sequence_log_prob(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 600, in compute_batch_sequence_log_prob
    logits = forward_pass()
             ^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 596, in forward_pass
    outputs = model(input_ids=batch_input_ids, attention_mask=attention_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 786, in convert_to_fp32
    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 118, in recursively_apply
    {
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 119, in <dictcomp>
    k: recursively_apply(
       ^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 778, in _convert_to_fp32
    return tensor.float()
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 2.33 GiB is free. Including non-PyTorch memory, this process has 76.91 GiB memory in use. Of the allocated memory 74.21 GiB is allocated by PyTorch, and 1.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[Group 9] Episodes 288-319 (expected)
Updating old model with current weights...
  ‚úì Old model updated
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (3, 6) towards (2, 5).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (8, 5) towards (9, 6).
Moving from (7, 2) towards (4, 2).
Moving from (1, 5) towards (3, 7).
Moving from (2, 5) towards (3, 7).
  [GPU2] Ep0: R=7.00, Steps=30, Time=106.97s
  [GPU3] Ep0: R=3.80, Steps=30, Time=107.00s
Moving from (6, 3) towards (9, 2).
Moving from (6, 2) towards (9, 2).
Moving from (1, 5) towards (3, 7).
  [GPU0] Ep0: R=4.20, Steps=30, Time=109.84s
Moving from (7, 2) towards (9, 2).
Moving from (2, 5) towards (3, 7).
  [GPU1] Ep0: R=6.00, Steps=30, Time=112.72s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (5, 7) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
  [GPU3] Ep1: R=4.00, Steps=30, Time=106.26s
  [GPU2] Ep1: R=3.80, Steps=30, Time=106.60s
  [GPU0] Ep1: R=4.00, Steps=30, Time=109.67s
Moving from (6, 3) towards (9, 2).
Moving from (6, 2) towards (9, 2).
Moving from (1, 5) towards (3, 7).
Moving from (7, 2) towards (9, 2).
Moving from (2, 5) towards (3, 7).
  [GPU1] Ep1: R=6.00, Steps=30, Time=111.89s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (8, 7) towards (12, 8).
Moving from (9, 7) towards (12, 8).
  [GPU3] Ep2: R=3.80, Steps=30, Time=105.95s
  [GPU2] Ep2: R=3.00, Steps=30, Time=106.37s
Moving from (10, 7) towards (12, 8).
  [GPU0] Ep2: R=3.00, Steps=30, Time=110.18s
  [GPU1] Ep2: R=3.80, Steps=30, Time=112.27s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (3, 6) towards (2, 5).
  [GPU3] Ep3: R=3.80, Steps=30, Time=106.34s
Moving from (7, 4) towards (5, 3).
  [GPU2] Ep3: R=5.20, Steps=30, Time=106.32s
  [GPU0] Ep3: R=4.20, Steps=30, Time=109.72s
  [GPU1] Ep3: R=4.20, Steps=30, Time=112.12s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU3] Ep4: R=2.40, Steps=30, Time=106.56s
  [GPU2] Ep4: R=3.80, Steps=30, Time=106.19s
  [GPU0] Ep4: R=3.80, Steps=30, Time=109.88s
  [GPU1] Ep4: R=3.00, Steps=30, Time=112.92s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (3, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU3] Ep5: R=3.80, Steps=30, Time=106.25s
  [GPU2] Ep5: R=2.40, Steps=30, Time=106.54s
  [GPU0] Ep5: R=3.00, Steps=30, Time=109.80s
  [GPU1] Ep5: R=3.80, Steps=30, Time=111.80s
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (3, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (5, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (2, 7) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (8, 5) towards (9, 6).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (7, 2) towards (4, 2).
Moving from (1, 5) towards (3, 7).
Moving from (2, 8) towards (2, 5).
Moving from (2, 5) towards (3, 7).
  [GPU3] Ep6: R=7.00, Steps=30, Time=106.13s
  [GPU2] Ep6: R=3.80, Steps=30, Time=106.82s
Moving from (2, 7) towards (2, 5).
Moving from (6, 3) towards (9, 2).
Moving from (6, 2) towards (9, 2).
Moving from (1, 5) towards (3, 7).
Moving from (7, 2) towards (9, 2).
Moving from (2, 5) towards (3, 7).
  [GPU0] Ep6: R=6.00, Steps=30, Time=109.48s
  [GPU1] Ep6: R=2.40, Steps=30, Time=111.64s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (2, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (7, 1) towards (9, 0).
Moving from (8, 4) towards (12, 4).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (9, 4) towards (12, 4).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (3, 4) towards (0, 4).
  [GPU3] Ep7: R=7.00, Steps=30, Time=106.66s
  [GPU2] Ep7: R=2.80, Steps=30, Time=106.20s
Moving from (4, 6) towards (2, 5).
Moving from (3, 6) towards (2, 5).
  [GPU0] Ep7: R=2.80, Steps=30, Time=109.33s
  [GPU1] Ep7: R=4.20, Steps=30, Time=111.63s
  ‚úì Collected 32 episodes ([8, 8, 8, 8] per GPU)
  üìù Logged episode 6 trajectory
  Reward: 4.12¬±1.33 [2.40, 7.00], Steps: 30.0
  Low-level actions (GPU0 sample): up:17% | down:22% | left:21% | right:20% | clean:16% | eat:0% | stay:0%

  Inner Optimization (4 epochs, minibatch=8):
Training step failed: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.96 GiB is free. Including non-PyTorch memory, this process has 77.28 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1243, in train
    loss, clip_fraction, n_samples = self.compute_loss_on_samples(micro_batch, model_device)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1012, in compute_loss_on_samples
    new_log_probs = self.compute_batch_sequence_log_prob(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 600, in compute_batch_sequence_log_prob
    logits = forward_pass()
             ^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 596, in forward_pass
    outputs = model(input_ids=batch_input_ids, attention_mask=attention_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 786, in convert_to_fp32
    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 118, in recursively_apply
    {
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 119, in <dictcomp>
    k: recursively_apply(
       ^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 778, in _convert_to_fp32
    return tensor.float()
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.96 GiB is free. Including non-PyTorch memory, this process has 77.28 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[Group 10] Episodes 320-351 (expected)
Updating old model with current weights...
  ‚úì Old model updated

  Sample generation (compound):
    Obs: You are at (5, 3). You see dirt at (7, 2) and dirt at (7, 4).
    Response (thinking+JSON): '{
    "action": "clean_at",
    "agent_id": 1,
    "args": {
        "coord_x": 7,
        "coord_y": 2
    }
}'
    ‚Üí Low-level action: right
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (8, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
  [GPU3] Ep0: R=3.20, Steps=30, Time=106.95s
  [GPU2] Ep0: R=4.20, Steps=30, Time=107.10s
[CUDA Memory - After episode rollout] Allocated: 30.43GB, Reserved: 77.60GB, Peak: 74.33GB
  [GPU1] Ep0: R=3.80, Steps=30, Time=111.62s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (8, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (8, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
  [GPU3] Ep1: R=2.40, Steps=30, Time=106.28s
  [GPU2] Ep1: R=4.00, Steps=30, Time=106.46s
  [GPU0] Ep1: R=3.20, Steps=30, Time=109.67s
  [GPU1] Ep1: R=3.20, Steps=30, Time=111.84s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (8, 7) towards (12, 8).
Moving from (9, 7) towards (12, 8).
Moving from (10, 7) towards (12, 8).
  [GPU3] Ep2: R=4.00, Steps=30, Time=106.13s
  [GPU2] Ep2: R=3.00, Steps=30, Time=107.08s
Moving from (6, 5) towards (2, 5).
Moving from (8, 8) towards (12, 8).
  [GPU0] Ep2: R=3.00, Steps=30, Time=109.92s
  [GPU1] Ep2: R=2.40, Steps=30, Time=111.75s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU3] Ep3: R=4.00, Steps=30, Time=105.98s
  [GPU2] Ep3: R=4.20, Steps=30, Time=106.82s
  [GPU0] Ep3: R=3.80, Steps=30, Time=109.59s
  [GPU1] Ep3: R=4.00, Steps=30, Time=111.25s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU3] Ep4: R=4.20, Steps=30, Time=106.14s
  [GPU2] Ep4: R=3.60, Steps=30, Time=106.76s
  [GPU0] Ep4: R=2.80, Steps=30, Time=109.33s
  [GPU1] Ep4: R=3.00, Steps=30, Time=111.32s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
  [GPU3] Ep5: R=4.20, Steps=30, Time=106.04s
  [GPU2] Ep5: R=4.20, Steps=30, Time=106.80s
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
  [GPU0] Ep5: R=4.00, Steps=30, Time=109.67s
Moving from (6, 3) towards (9, 2).
Moving from (6, 2) towards (9, 2).
Moving from (1, 5) towards (3, 7).
Moving from (7, 2) towards (9, 2).
Moving from (2, 5) towards (3, 7).
  [GPU1] Ep5: R=6.00, Steps=30, Time=112.75s
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (3, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU3] Ep6: R=2.40, Steps=30, Time=106.08s
  [GPU2] Ep6: R=4.20, Steps=30, Time=106.94s
  [GPU0] Ep6: R=3.80, Steps=30, Time=109.37s
  [GPU1] Ep6: R=2.40, Steps=30, Time=111.34s
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU3] Ep7: R=2.40, Steps=30, Time=106.39s
  [GPU2] Ep7: R=2.80, Steps=30, Time=106.76s
  [GPU0] Ep7: R=4.20, Steps=30, Time=109.65s
  [GPU1] Ep7: R=2.40, Steps=30, Time=112.12s
  ‚úì Collected 32 episodes ([8, 8, 8, 8] per GPU)
  üìù Logged episode 5 trajectory
  Reward: 3.48¬±0.84 [2.40, 6.00], Steps: 30.0
  Low-level actions (GPU0 sample): up:18% | down:23% | left:21% | right:20% | clean:16% | eat:0% | stay:0%

  Inner Optimization (4 epochs, minibatch=8):
Training step failed: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 2.33 GiB is free. Including non-PyTorch memory, this process has 76.91 GiB memory in use. Of the allocated memory 74.21 GiB is allocated by PyTorch, and 1.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1243, in train
    loss, clip_fraction, n_samples = self.compute_loss_on_samples(micro_batch, model_device)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1012, in compute_loss_on_samples
    new_log_probs = self.compute_batch_sequence_log_prob(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 600, in compute_batch_sequence_log_prob
    logits = forward_pass()
             ^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 596, in forward_pass
    outputs = model(input_ids=batch_input_ids, attention_mask=attention_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 786, in convert_to_fp32
    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 118, in recursively_apply
    {
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 119, in <dictcomp>
    k: recursively_apply(
       ^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 778, in _convert_to_fp32
    return tensor.float()
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 2.33 GiB is free. Including non-PyTorch memory, this process has 76.91 GiB memory in use. Of the allocated memory 74.21 GiB is allocated by PyTorch, and 1.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[Group 11] Episodes 352-383 (expected)
Updating old model with current weights...
  ‚úì Old model updated
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (3, 8) towards (1, 8).
  [GPU3] Ep0: R=3.00, Steps=30, Time=107.52s
  [GPU2] Ep0: R=4.00, Steps=30, Time=107.62s
  [GPU0] Ep0: R=3.80, Steps=30, Time=110.41s
  [GPU1] Ep0: R=4.00, Steps=30, Time=111.83s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (3, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (3, 5) towards (3, 7).
Moving from (6, 3) towards (4, 2).
Moving from (6, 2) towards (4, 2).
Moving from (5, 3) towards (4, 2).
  [GPU3] Ep1: R=5.80, Steps=30, Time=106.65s
  [GPU2] Ep1: R=2.40, Steps=30, Time=106.77s
  [GPU0] Ep1: R=3.80, Steps=30, Time=109.15s
  [GPU1] Ep1: R=3.80, Steps=30, Time=111.55s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (8, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
  [GPU3] Ep2: R=2.80, Steps=30, Time=106.27s
  [GPU2] Ep2: R=3.20, Steps=30, Time=106.77s
  [GPU0] Ep2: R=4.00, Steps=30, Time=108.96s
  [GPU1] Ep2: R=3.00, Steps=30, Time=111.79s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (8, 8) towards (12, 8).
  [GPU3] Ep3: R=4.00, Steps=30, Time=106.71s
  [GPU2] Ep3: R=3.00, Steps=30, Time=106.78s
  [GPU0] Ep3: R=4.20, Steps=30, Time=109.17s
  [GPU1] Ep3: R=4.00, Steps=30, Time=112.19s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (3, 6) towards (2, 5).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU3] Ep4: R=2.40, Steps=30, Time=106.21s
  [GPU2] Ep4: R=3.80, Steps=30, Time=106.03s
  [GPU0] Ep4: R=4.00, Steps=30, Time=108.80s
  [GPU1] Ep4: R=3.00, Steps=30, Time=111.37s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU3] Ep5: R=2.40, Steps=30, Time=106.78s
  [GPU2] Ep5: R=3.00, Steps=30, Time=106.30s
  [GPU0] Ep5: R=3.80, Steps=30, Time=109.00s
  [GPU1] Ep5: R=4.00, Steps=30, Time=111.31s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
  [GPU2] Ep6: R=2.80, Steps=30, Time=106.15s
  [GPU3] Ep6: R=4.20, Steps=30, Time=106.60s
Moving from (3, 5) towards (5, 5).
Moving from (8, 5) towards (5, 5).
Moving from (8, 6) towards (5, 6).
  [GPU0] Ep6: R=6.00, Steps=30, Time=109.12s
  [GPU1] Ep6: R=2.40, Steps=30, Time=111.51s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 3) towards (9, 2).
Moving from (6, 2) towards (9, 2).
Moving from (1, 5) towards (3, 7).
Moving from (7, 2) towards (9, 2).
Moving from (2, 5) towards (3, 7).
  [GPU2] Ep7: R=6.00, Steps=30, Time=106.33s
  [GPU3] Ep7: R=3.80, Steps=30, Time=106.86s
  [GPU0] Ep7: R=3.80, Steps=30, Time=109.55s
  [GPU1] Ep7: R=2.80, Steps=30, Time=112.18s
  ‚úì Collected 32 episodes ([8, 8, 8, 8] per GPU)
  üìù Logged episode 1 trajectory
  Reward: 3.66¬±0.95 [2.40, 6.00], Steps: 30.0
  Low-level actions (GPU0 sample): up:17% | down:19% | left:23% | right:21% | clean:16% | eat:1% | stay:0%

  Inner Optimization (4 epochs, minibatch=8):
Training step failed: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 2.30 GiB is free. Including non-PyTorch memory, this process has 76.95 GiB memory in use. Of the allocated memory 74.21 GiB is allocated by PyTorch, and 1.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1243, in train
    loss, clip_fraction, n_samples = self.compute_loss_on_samples(micro_batch, model_device)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1012, in compute_loss_on_samples
    new_log_probs = self.compute_batch_sequence_log_prob(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 600, in compute_batch_sequence_log_prob
    logits = forward_pass()
             ^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 596, in forward_pass
    outputs = model(input_ids=batch_input_ids, attention_mask=attention_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 786, in convert_to_fp32
    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 118, in recursively_apply
    {
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 119, in <dictcomp>
    k: recursively_apply(
       ^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 778, in _convert_to_fp32
    return tensor.float()
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 2.30 GiB is free. Including non-PyTorch memory, this process has 76.95 GiB memory in use. Of the allocated memory 74.21 GiB is allocated by PyTorch, and 1.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[Group 12] Episodes 384-415 (expected)
Updating old model with current weights...
  ‚úì Old model updated
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (3, 8) towards (1, 8).
  [GPU3] Ep0: R=3.80, Steps=30, Time=106.51s
  [GPU2] Ep0: R=3.80, Steps=30, Time=106.64s
  [GPU0] Ep0: R=4.00, Steps=30, Time=109.49s
  [GPU1] Ep0: R=3.20, Steps=30, Time=111.40s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 6) towards (2, 5).
  [GPU3] Ep1: R=4.00, Steps=30, Time=106.10s
  [GPU2] Ep1: R=3.80, Steps=30, Time=106.69s
  [GPU0] Ep1: R=4.00, Steps=30, Time=109.22s
  [GPU1] Ep1: R=4.20, Steps=30, Time=111.47s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU3] Ep2: R=3.80, Steps=30, Time=106.33s
  [GPU2] Ep2: R=3.80, Steps=30, Time=106.31s
  [GPU0] Ep2: R=4.00, Steps=30, Time=109.00s
  [GPU1] Ep2: R=3.80, Steps=30, Time=111.36s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
  [GPU3] Ep3: R=3.00, Steps=30, Time=106.34s
  [GPU2] Ep3: R=4.00, Steps=30, Time=106.28s
Moving from (7, 1) towards (9, 0).
  [GPU0] Ep3: R=3.80, Steps=30, Time=109.04s
Moving from (8, 4) towards (12, 4).
Moving from (3, 4) towards (0, 4).
  [GPU1] Ep3: R=7.00, Steps=30, Time=111.51s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (9, 5) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (8, 7) towards (12, 8).
  [GPU3] Ep4: R=3.00, Steps=30, Time=106.51s
  [GPU2] Ep4: R=3.00, Steps=30, Time=106.63s
  [GPU0] Ep4: R=4.00, Steps=30, Time=109.76s
  [GPU1] Ep4: R=3.80, Steps=30, Time=111.44s
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (3, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU3] Ep5: R=4.20, Steps=30, Time=106.09s
  [GPU2] Ep5: R=3.80, Steps=30, Time=106.22s
  [GPU0] Ep5: R=4.20, Steps=30, Time=110.00s
  [GPU1] Ep5: R=3.00, Steps=30, Time=111.95s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
  [GPU3] Ep6: R=3.80, Steps=30, Time=106.22s
  [GPU2] Ep6: R=4.00, Steps=30, Time=106.65s
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
  [GPU0] Ep6: R=2.80, Steps=30, Time=108.87s
Moving from (8, 5) towards (5, 5).
Moving from (8, 6) towards (5, 6).
  [GPU1] Ep6: R=6.00, Steps=30, Time=111.06s
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (2, 7) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (8, 5) towards (9, 6).
Moving from (5, 8) towards (1, 8).
Moving from (7, 2) towards (4, 2).
Moving from (1, 5) towards (3, 7).
Moving from (2, 5) towards (3, 7).
  [GPU3] Ep7: R=7.00, Steps=30, Time=106.84s
  [GPU2] Ep7: R=4.20, Steps=30, Time=106.99s
  [GPU0] Ep7: R=2.40, Steps=30, Time=109.40s
Moving from (8, 7) towards (12, 8).
  [GPU1] Ep7: R=3.00, Steps=30, Time=111.41s
  ‚úì Collected 32 episodes ([8, 8, 8, 8] per GPU)
  üìù Logged episode 3 trajectory
  Reward: 3.94¬±1.02 [2.40, 7.00], Steps: 30.0
  Low-level actions (GPU0 sample): up:17% | down:23% | left:22% | right:20% | clean:16% | eat:0% | stay:0%

  Inner Optimization (4 epochs, minibatch=8):
Training step failed: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 2.26 GiB is free. Including non-PyTorch memory, this process has 76.98 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1243, in train
    loss, clip_fraction, n_samples = self.compute_loss_on_samples(micro_batch, model_device)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1012, in compute_loss_on_samples
    new_log_probs = self.compute_batch_sequence_log_prob(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 600, in compute_batch_sequence_log_prob
    logits = forward_pass()
             ^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 596, in forward_pass
    outputs = model(input_ids=batch_input_ids, attention_mask=attention_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 786, in convert_to_fp32
    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 118, in recursively_apply
    {
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 119, in <dictcomp>
    k: recursively_apply(
       ^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 778, in _convert_to_fp32
    return tensor.float()
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 2.26 GiB is free. Including non-PyTorch memory, this process has 76.98 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[Group 13] Episodes 416-447 (expected)
Updating old model with current weights...
  ‚úì Old model updated
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 6) towards (2, 5).
Moving from (3, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (2, 5) towards (3, 6).
  [GPU2] Ep0: R=3.80, Steps=30, Time=107.26s
  [GPU3] Ep0: R=4.20, Steps=30, Time=107.52s
  [GPU0] Ep0: R=2.80, Steps=30, Time=110.16s
Moving from (3, 6) towards (4, 7).
  [GPU1] Ep0: R=5.80, Steps=30, Time=112.05s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (3, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
  [GPU2] Ep1: R=3.80, Steps=30, Time=106.70s
  [GPU3] Ep1: R=3.80, Steps=30, Time=106.49s
  [GPU0] Ep1: R=4.00, Steps=30, Time=109.48s
  [GPU1] Ep1: R=2.40, Steps=30, Time=111.72s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (8, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
  [GPU3] Ep2: R=4.00, Steps=30, Time=106.23s
  [GPU2] Ep2: R=3.20, Steps=30, Time=106.88s
  [GPU0] Ep2: R=2.40, Steps=30, Time=109.71s
  [GPU1] Ep2: R=4.20, Steps=30, Time=111.55s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
  [GPU2] Ep3: R=3.20, Steps=30, Time=105.25s
  [GPU3] Ep3: R=4.00, Steps=30, Time=106.56s
  [GPU0] Ep3: R=4.00, Steps=30, Time=109.89s
  [GPU1] Ep3: R=2.80, Steps=30, Time=110.89s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (2, 7) towards (2, 5).
Moving from (3, 5) towards (5, 5).
Moving from (8, 5) towards (5, 5).
Moving from (8, 6) towards (5, 6).
  [GPU2] Ep4: R=4.00, Steps=30, Time=106.37s
  [GPU3] Ep4: R=6.00, Steps=30, Time=106.26s
  [GPU0] Ep4: R=4.00, Steps=30, Time=109.72s
  [GPU1] Ep4: R=2.40, Steps=30, Time=111.50s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU2] Ep5: R=4.00, Steps=30, Time=106.68s
  [GPU3] Ep5: R=4.20, Steps=30, Time=106.30s
Moving from (6, 5) towards (2, 5).
Moving from (8, 8) towards (12, 8).
  [GPU0] Ep5: R=3.20, Steps=30, Time=109.89s
  [GPU1] Ep5: R=2.80, Steps=30, Time=110.93s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU2] Ep6: R=4.00, Steps=30, Time=106.42s
  [GPU3] Ep6: R=3.80, Steps=30, Time=106.15s
  [GPU0] Ep6: R=3.00, Steps=30, Time=109.58s
  [GPU1] Ep6: R=4.20, Steps=30, Time=111.36s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU3] Ep7: R=3.00, Steps=30, Time=106.44s
  [GPU2] Ep7: R=3.00, Steps=30, Time=106.48s
  [GPU0] Ep7: R=4.20, Steps=30, Time=109.10s
  [GPU1] Ep7: R=2.80, Steps=30, Time=111.27s
  ‚úì Collected 32 episodes ([8, 8, 8, 8] per GPU)
  üìù Logged episode 3 trajectory
  Reward: 3.66¬±0.85 [2.40, 6.00], Steps: 30.0
  Low-level actions (GPU0 sample): up:18% | down:24% | left:20% | right:19% | clean:16% | eat:0% | stay:0%

  Inner Optimization (4 epochs, minibatch=8):
Training step failed: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.89 GiB is free. Including non-PyTorch memory, this process has 77.35 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1243, in train
    loss, clip_fraction, n_samples = self.compute_loss_on_samples(micro_batch, model_device)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1012, in compute_loss_on_samples
    new_log_probs = self.compute_batch_sequence_log_prob(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 600, in compute_batch_sequence_log_prob
    logits = forward_pass()
             ^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 596, in forward_pass
    outputs = model(input_ids=batch_input_ids, attention_mask=attention_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 786, in convert_to_fp32
    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 118, in recursively_apply
    {
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 119, in <dictcomp>
    k: recursively_apply(
       ^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 778, in _convert_to_fp32
    return tensor.float()
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.89 GiB is free. Including non-PyTorch memory, this process has 77.35 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[Group 14] Episodes 448-479 (expected)
Updating old model with current weights...
  ‚úì Old model updated
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (8, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (9, 5) towards (12, 8).
Moving from (7, 8) towards (12, 8).
  [GPU2] Ep0: R=2.80, Steps=30, Time=106.60s
  [GPU3] Ep0: R=3.20, Steps=30, Time=107.61s
  [GPU0] Ep0: R=4.00, Steps=30, Time=110.41s
  [GPU1] Ep0: R=3.00, Steps=30, Time=110.45s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU2] Ep1: R=2.40, Steps=30, Time=106.82s
  [GPU3] Ep1: R=3.00, Steps=30, Time=106.58s
  [GPU0] Ep1: R=4.00, Steps=30, Time=109.28s
  [GPU1] Ep1: R=3.00, Steps=30, Time=109.87s
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (3, 5) towards (3, 7).
Moving from (6, 3) towards (4, 2).
Moving from (6, 2) towards (4, 2).
Moving from (5, 3) towards (4, 2).
  [GPU2] Ep2: R=5.80, Steps=30, Time=106.86s
  [GPU3] Ep2: R=4.00, Steps=30, Time=106.43s
  [GPU0] Ep2: R=2.40, Steps=30, Time=109.72s
  [GPU1] Ep2: R=3.80, Steps=30, Time=111.33s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (2, 5) towards (3, 6).
Moving from (3, 6) towards (4, 7).
  [GPU3] Ep3: R=5.80, Steps=30, Time=106.19s
  [GPU2] Ep3: R=4.00, Steps=30, Time=106.64s
  [GPU0] Ep3: R=3.80, Steps=30, Time=109.66s
  [GPU1] Ep3: R=3.00, Steps=30, Time=110.82s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (8, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
  [GPU3] Ep4: R=3.20, Steps=30, Time=106.83s
  [GPU2] Ep4: R=4.00, Steps=30, Time=106.97s
  [GPU0] Ep4: R=4.20, Steps=30, Time=109.26s
  [GPU1] Ep4: R=4.20, Steps=30, Time=110.78s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU2] Ep5: R=3.00, Steps=30, Time=106.50s
  [GPU3] Ep5: R=4.00, Steps=30, Time=107.19s
  [GPU0] Ep5: R=3.00, Steps=30, Time=108.80s
  [GPU1] Ep5: R=2.40, Steps=30, Time=111.18s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
  [GPU2] Ep6: R=3.00, Steps=30, Time=106.52s
  [GPU3] Ep6: R=4.00, Steps=30, Time=106.89s
Moving from (8, 7) towards (12, 8).
Moving from (6, 3) towards (9, 2).
Moving from (6, 2) towards (9, 2).
Moving from (1, 5) towards (3, 7).
Moving from (9, 7) towards (12, 8).
Moving from (7, 2) towards (9, 2).
Moving from (2, 5) towards (3, 7).
  [GPU0] Ep6: R=6.00, Steps=30, Time=109.32s
Moving from (10, 7) towards (12, 8).
  [GPU1] Ep6: R=3.00, Steps=30, Time=111.64s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (9, 5) towards (12, 8).
Moving from (5, 8) towards (1, 8).
  [GPU2] Ep7: R=3.00, Steps=30, Time=106.97s
  [GPU3] Ep7: R=4.20, Steps=30, Time=107.48s
  [GPU0] Ep7: R=3.00, Steps=30, Time=109.36s
  [GPU1] Ep7: R=3.80, Steps=30, Time=110.91s
  ‚úì Collected 32 episodes ([8, 8, 8, 8] per GPU)
  üìù Logged episode 2 trajectory
  Reward: 3.62¬±0.93 [2.40, 6.00], Steps: 30.0
  Low-level actions (GPU0 sample): up:17% | down:24% | left:21% | right:19% | clean:16% | eat:0% | stay:0%

  Inner Optimization (4 epochs, minibatch=8):
Training step failed: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.94 GiB is free. Including non-PyTorch memory, this process has 77.30 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1243, in train
    loss, clip_fraction, n_samples = self.compute_loss_on_samples(micro_batch, model_device)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1012, in compute_loss_on_samples
    new_log_probs = self.compute_batch_sequence_log_prob(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 600, in compute_batch_sequence_log_prob
    logits = forward_pass()
             ^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 596, in forward_pass
    outputs = model(input_ids=batch_input_ids, attention_mask=attention_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 786, in convert_to_fp32
    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 118, in recursively_apply
    {
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 119, in <dictcomp>
    k: recursively_apply(
       ^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 778, in _convert_to_fp32
    return tensor.float()
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.94 GiB is free. Including non-PyTorch memory, this process has 77.30 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[Group 15] Episodes 480-511 (expected)
Updating old model with current weights...
  ‚úì Old model updated

  Sample generation (compound):
    Obs: You are at (5, 3). You see dirt at (7, 2) and dirt at (7, 4).
    Response (thinking+JSON): '{
    "action": "clean_at",
    "agent_id": 1,
    "args": {
        "coord_x": 7,
        "coord_y": 2
    }
}'
    ‚Üí Low-level action: right
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (3, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (7, 1) towards (9, 0).
Moving from (2, 5) towards (3, 4).
Moving from (3, 5) towards (5, 5).
Moving from (6, 4) towards (3, 4).
Moving from (8, 5) towards (5, 5).
Moving from (8, 6) towards (5, 6).
  [GPU2] Ep0: R=6.00, Steps=30, Time=107.21s
Moving from (4, 4) towards (4, 1).
Moving from (3, 4) towards (0, 4).
  [GPU3] Ep0: R=7.00, Steps=30, Time=108.17s
Moving from (7, 4) towards (5, 3).
[CUDA Memory - After episode rollout] Allocated: 30.43GB, Reserved: 77.54GB, Peak: 74.33GB
  [GPU1] Ep0: R=3.80, Steps=30, Time=111.70s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (5, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
  [GPU2] Ep1: R=4.00, Steps=30, Time=106.75s
  [GPU3] Ep1: R=3.80, Steps=30, Time=107.69s
  [GPU0] Ep1: R=2.40, Steps=30, Time=109.29s
  [GPU1] Ep1: R=3.00, Steps=30, Time=110.13s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU2] Ep2: R=2.40, Steps=30, Time=106.65s
  [GPU3] Ep2: R=3.00, Steps=30, Time=106.41s
  [GPU0] Ep2: R=2.40, Steps=30, Time=109.42s
  [GPU1] Ep2: R=3.00, Steps=30, Time=111.49s
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (2, 5) towards (1, 3).
Moving from (1, 5) towards (0, 4).
  [GPU2] Ep3: R=3.80, Steps=30, Time=106.48s
  [GPU3] Ep3: R=3.80, Steps=30, Time=106.43s
Moving from (0, 4) towards (1, 3).
  [GPU0] Ep3: R=4.20, Steps=30, Time=109.17s
  [GPU1] Ep3: R=7.20, Steps=30, Time=110.91s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (8, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
  [GPU2] Ep4: R=4.20, Steps=30, Time=106.55s
  [GPU3] Ep4: R=3.20, Steps=30, Time=106.59s
  [GPU0] Ep4: R=3.80, Steps=30, Time=109.77s
  [GPU1] Ep4: R=4.00, Steps=30, Time=111.32s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
  [GPU2] Ep5: R=3.00, Steps=30, Time=106.24s
Moving from (2, 8) towards (2, 5).
  [GPU3] Ep5: R=3.00, Steps=30, Time=106.35s
Moving from (2, 7) towards (2, 5).
  [GPU0] Ep5: R=4.00, Steps=30, Time=109.60s
Moving from (3, 5) towards (3, 7).
Moving from (6, 3) towards (4, 2).
Moving from (6, 2) towards (4, 2).
Moving from (5, 3) towards (4, 2).
  [GPU1] Ep5: R=5.80, Steps=30, Time=111.00s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (8, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
  [GPU2] Ep6: R=3.20, Steps=30, Time=106.51s
Moving from (2, 5) towards (1, 3).
  [GPU3] Ep6: R=3.20, Steps=30, Time=107.08s
Moving from (1, 5) towards (0, 4).
  [GPU0] Ep6: R=3.60, Steps=30, Time=109.77s
  [GPU1] Ep6: R=7.20, Steps=30, Time=111.08s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU2] Ep7: R=3.00, Steps=30, Time=106.83s
  [GPU3] Ep7: R=2.40, Steps=30, Time=107.13s
  [GPU0] Ep7: R=3.00, Steps=30, Time=107.86s
  [GPU1] Ep7: R=4.20, Steps=30, Time=111.10s
  ‚úì Collected 32 episodes ([8, 8, 8, 8] per GPU)
  üìù Logged episode 6 trajectory
  Reward: 3.93¬±1.36 [2.40, 7.20], Steps: 30.0
  Low-level actions (GPU0 sample): up:18% | down:21% | left:22% | right:20% | clean:15% | eat:0% | stay:0%

  Inner Optimization (4 epochs, minibatch=8):
Training step failed: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 77.27 GiB memory in use. Of the allocated memory 74.21 GiB is allocated by PyTorch, and 1.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1243, in train
    loss, clip_fraction, n_samples = self.compute_loss_on_samples(micro_batch, model_device)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1012, in compute_loss_on_samples
    new_log_probs = self.compute_batch_sequence_log_prob(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 600, in compute_batch_sequence_log_prob
    logits = forward_pass()
             ^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 596, in forward_pass
    outputs = model(input_ids=batch_input_ids, attention_mask=attention_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 786, in convert_to_fp32
    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 118, in recursively_apply
    {
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 119, in <dictcomp>
    k: recursively_apply(
       ^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 778, in _convert_to_fp32
    return tensor.float()
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 77.27 GiB memory in use. Of the allocated memory 74.21 GiB is allocated by PyTorch, and 1.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[Group 16] Episodes 512-543 (expected)
Updating old model with current weights...
  ‚úì Old model updated
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (3, 8) towards (1, 8).
Moving from (3, 6) towards (2, 5).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (3, 5) towards (3, 7).
Moving from (6, 3) towards (4, 2).
Moving from (6, 2) towards (4, 2).
Moving from (5, 3) towards (4, 2).
  [GPU2] Ep0: R=5.80, Steps=30, Time=107.74s
  [GPU3] Ep0: R=3.80, Steps=30, Time=108.61s
  [GPU0] Ep0: R=3.80, Steps=30, Time=109.05s
  [GPU1] Ep0: R=3.00, Steps=30, Time=111.32s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (8, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (3, 5) towards (3, 7).
Moving from (6, 3) towards (4, 2).
Moving from (6, 2) towards (4, 2).
  [GPU2] Ep1: R=3.20, Steps=30, Time=106.89s
Moving from (5, 3) towards (4, 2).
  [GPU3] Ep1: R=5.80, Steps=30, Time=107.79s
  [GPU0] Ep1: R=2.80, Steps=30, Time=108.47s
  [GPU1] Ep1: R=4.00, Steps=30, Time=111.00s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (8, 5) towards (9, 6).
Moving from (7, 2) towards (4, 2).
Moving from (1, 5) towards (3, 7).
  [GPU2] Ep2: R=2.40, Steps=30, Time=106.62s
Moving from (2, 5) towards (3, 7).
  [GPU3] Ep2: R=7.00, Steps=30, Time=107.54s
  [GPU0] Ep2: R=3.80, Steps=30, Time=108.62s
  [GPU1] Ep2: R=2.80, Steps=30, Time=110.55s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
  [GPU2] Ep3: R=3.80, Steps=30, Time=106.85s
  [GPU3] Ep3: R=4.00, Steps=30, Time=107.65s
  [GPU0] Ep3: R=3.00, Steps=30, Time=108.89s
Moving from (3, 5) towards (3, 7).
Moving from (6, 3) towards (4, 2).
Moving from (6, 2) towards (4, 2).
Moving from (5, 3) towards (4, 2).
  [GPU1] Ep3: R=5.80, Steps=30, Time=110.73s
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (8, 8) towards (12, 8).
Moving from (8, 5) towards (9, 6).
  [GPU2] Ep4: R=3.00, Steps=30, Time=107.24s
Moving from (7, 2) towards (4, 2).
  [GPU3] Ep4: R=4.00, Steps=30, Time=107.59s
Moving from (1, 5) towards (3, 7).
Moving from (2, 5) towards (3, 7).
  [GPU0] Ep4: R=7.00, Steps=30, Time=109.35s
  [GPU1] Ep4: R=4.00, Steps=30, Time=110.81s
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
  [GPU2] Ep5: R=4.00, Steps=30, Time=106.93s
  [GPU3] Ep5: R=4.20, Steps=30, Time=107.42s
Moving from (6, 3) towards (9, 2).
Moving from (6, 2) towards (9, 2).
Moving from (1, 5) towards (3, 7).
Moving from (7, 2) towards (9, 2).
Moving from (2, 5) towards (3, 7).
  [GPU0] Ep5: R=6.00, Steps=30, Time=108.91s
  [GPU1] Ep5: R=4.00, Steps=30, Time=110.77s
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU2] Ep6: R=4.00, Steps=30, Time=106.66s
  [GPU3] Ep6: R=3.60, Steps=30, Time=106.41s
  [GPU0] Ep6: R=2.40, Steps=30, Time=109.12s
  [GPU1] Ep6: R=3.00, Steps=30, Time=109.73s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU2] Ep7: R=2.80, Steps=30, Time=106.43s
  [GPU3] Ep7: R=3.80, Steps=30, Time=106.42s
  [GPU0] Ep7: R=4.00, Steps=30, Time=108.88s
  [GPU1] Ep7: R=2.80, Steps=30, Time=110.90s
  ‚úì Collected 32 episodes ([8, 8, 8, 8] per GPU)
  üìù Logged episode 0 trajectory
  Reward: 3.98¬±1.24 [2.40, 7.00], Steps: 30.0
  Low-level actions (GPU0 sample): up:18% | down:22% | left:23% | right:19% | clean:15% | eat:1% | stay:0%

  Inner Optimization (4 epochs, minibatch=8):
Training step failed: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.90 GiB is free. Including non-PyTorch memory, this process has 77.34 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1243, in train
    loss, clip_fraction, n_samples = self.compute_loss_on_samples(micro_batch, model_device)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1012, in compute_loss_on_samples
    new_log_probs = self.compute_batch_sequence_log_prob(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 600, in compute_batch_sequence_log_prob
    logits = forward_pass()
             ^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 596, in forward_pass
    outputs = model(input_ids=batch_input_ids, attention_mask=attention_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 786, in convert_to_fp32
    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 118, in recursively_apply
    {
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 119, in <dictcomp>
    k: recursively_apply(
       ^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 778, in _convert_to_fp32
    return tensor.float()
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.90 GiB is free. Including non-PyTorch memory, this process has 77.34 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[Group 17] Episodes 544-575 (expected)
Updating old model with current weights...
  ‚úì Old model updated
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 3) towards (5, 2).
Moving from (9, 4) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 3) towards (5, 2).
  [GPU2] Ep0: R=2.80, Steps=30, Time=106.90s
  [GPU3] Ep0: R=4.00, Steps=30, Time=108.04s
  [GPU0] Ep0: R=3.00, Steps=30, Time=110.12s
  [GPU1] Ep0: R=7.00, Steps=30, Time=112.06s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (6, 3) towards (9, 2).
Moving from (6, 2) towards (9, 2).
Moving from (1, 5) towards (3, 7).
  [GPU2] Ep1: R=4.20, Steps=30, Time=107.24s
Moving from (7, 2) towards (9, 2).
Moving from (2, 5) towards (3, 7).
  [GPU3] Ep1: R=6.00, Steps=30, Time=107.77s
  [GPU0] Ep1: R=3.00, Steps=30, Time=109.26s
  [GPU1] Ep1: R=4.00, Steps=30, Time=111.47s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
  [GPU2] Ep2: R=4.00, Steps=30, Time=106.80s
  [GPU3] Ep2: R=4.00, Steps=30, Time=107.25s
  [GPU0] Ep2: R=3.80, Steps=30, Time=109.37s
  [GPU1] Ep2: R=4.20, Steps=30, Time=111.14s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (8, 7) towards (12, 8).
  [GPU2] Ep3: R=3.00, Steps=30, Time=106.75s
  [GPU3] Ep3: R=3.00, Steps=30, Time=106.51s
  [GPU0] Ep3: R=3.80, Steps=30, Time=108.43s
  [GPU1] Ep3: R=3.00, Steps=30, Time=111.26s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (8, 5) towards (9, 6).
Moving from (7, 2) towards (4, 2).
Moving from (1, 5) towards (3, 7).
Moving from (2, 5) towards (3, 7).
  [GPU2] Ep4: R=7.00, Steps=30, Time=107.45s
  [GPU3] Ep4: R=2.80, Steps=30, Time=106.49s
  [GPU0] Ep4: R=4.00, Steps=30, Time=109.15s
  [GPU1] Ep4: R=3.80, Steps=30, Time=111.01s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 8) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (2, 7) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (7, 4) towards (3, 4).
Moving from (7, 1) towards (9, 0).
Moving from (2, 5) towards (3, 6).
Moving from (6, 4) towards (3, 4).
Moving from (4, 4) towards (4, 1).
Moving from (3, 4) towards (0, 4).
  [GPU2] Ep5: R=7.00, Steps=30, Time=106.65s
Moving from (3, 6) towards (4, 7).
  [GPU3] Ep5: R=5.80, Steps=30, Time=106.39s
  [GPU0] Ep5: R=2.80, Steps=30, Time=109.64s
Moving from (8, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
  [GPU1] Ep5: R=3.20, Steps=30, Time=111.35s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU2] Ep6: R=3.80, Steps=30, Time=106.57s
  [GPU3] Ep6: R=2.80, Steps=30, Time=106.09s
  [GPU0] Ep6: R=3.80, Steps=30, Time=109.25s
  [GPU1] Ep6: R=4.00, Steps=30, Time=110.94s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
  [GPU2] Ep7: R=3.00, Steps=30, Time=106.93s
  [GPU3] Ep7: R=3.00, Steps=30, Time=106.80s
  [GPU0] Ep7: R=3.80, Steps=30, Time=109.32s
  [GPU1] Ep7: R=3.00, Steps=30, Time=111.14s
  ‚úì Collected 32 episodes ([8, 8, 8, 8] per GPU)
  üìù Logged episode 5 trajectory
  Reward: 3.95¬±1.26 [2.80, 7.00], Steps: 30.0
  Low-level actions (GPU0 sample): up:17% | down:20% | left:24% | right:21% | clean:15% | eat:0% | stay:0%

  Inner Optimization (4 epochs, minibatch=8):
Training step failed: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1243, in train
    loss, clip_fraction, n_samples = self.compute_loss_on_samples(micro_batch, model_device)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1012, in compute_loss_on_samples
    new_log_probs = self.compute_batch_sequence_log_prob(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 600, in compute_batch_sequence_log_prob
    logits = forward_pass()
             ^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 596, in forward_pass
    outputs = model(input_ids=batch_input_ids, attention_mask=attention_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 786, in convert_to_fp32
    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 118, in recursively_apply
    {
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 119, in <dictcomp>
    k: recursively_apply(
       ^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 778, in _convert_to_fp32
    return tensor.float()
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.95 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[Group 18] Episodes 576-607 (expected)
Updating old model with current weights...
  ‚úì Old model updated
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (8, 8) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (8, 8) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (8, 8) towards (12, 8).
  [GPU2] Ep0: R=2.80, Steps=30, Time=106.94s
  [GPU3] Ep0: R=3.20, Steps=30, Time=107.81s
Moving from (9, 8) towards (12, 8).
  [GPU0] Ep0: R=3.00, Steps=30, Time=110.49s
  [GPU1] Ep0: R=3.20, Steps=30, Time=111.93s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU2] Ep1: R=2.80, Steps=30, Time=106.91s
  [GPU3] Ep1: R=3.00, Steps=30, Time=106.77s
  [GPU0] Ep1: R=4.00, Steps=30, Time=109.48s
  [GPU1] Ep1: R=3.20, Steps=30, Time=110.00s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (7, 4) towards (3, 4).
Moving from (7, 1) towards (9, 0).
Moving from (6, 4) towards (3, 4).
  [GPU2] Ep2: R=3.00, Steps=30, Time=107.40s
Moving from (4, 4) towards (4, 1).
Moving from (3, 4) towards (0, 4).
  [GPU3] Ep2: R=7.00, Steps=30, Time=107.21s
Moving from (3, 5) towards (5, 5).
Moving from (8, 5) towards (5, 5).
Moving from (8, 6) towards (5, 6).
  [GPU0] Ep2: R=6.00, Steps=30, Time=109.35s
  [GPU1] Ep2: R=5.20, Steps=30, Time=110.68s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
  [GPU2] Ep3: R=3.20, Steps=30, Time=107.63s
  [GPU3] Ep3: R=4.00, Steps=30, Time=107.78s
Moving from (3, 5) towards (3, 7).
  [GPU0] Ep3: R=3.00, Steps=30, Time=109.22s
Moving from (6, 3) towards (4, 2).
Moving from (6, 2) towards (4, 2).
Moving from (5, 3) towards (4, 2).
  [GPU1] Ep3: R=5.80, Steps=30, Time=110.91s
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (3, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU2] Ep4: R=3.80, Steps=30, Time=107.48s
Moving from (6, 8) towards (1, 8).
  [GPU3] Ep4: R=3.80, Steps=30, Time=107.67s
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
  [GPU0] Ep4: R=3.00, Steps=30, Time=109.76s
  [GPU1] Ep4: R=4.00, Steps=30, Time=110.87s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
  [GPU3] Ep5: R=2.80, Steps=30, Time=106.59s
  [GPU2] Ep5: R=4.00, Steps=30, Time=107.57s
  [GPU0] Ep5: R=2.80, Steps=30, Time=108.23s
Moving from (3, 5) towards (5, 5).
Moving from (8, 5) towards (5, 5).
Moving from (8, 6) towards (5, 6).
  [GPU1] Ep5: R=6.00, Steps=30, Time=110.88s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (3, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (8, 7) towards (12, 8).
Moving from (9, 7) towards (12, 8).
Moving from (10, 7) towards (12, 8).
  [GPU2] Ep6: R=2.80, Steps=30, Time=107.33s
  [GPU3] Ep6: R=3.00, Steps=30, Time=107.71s
  [GPU0] Ep6: R=3.80, Steps=30, Time=109.41s
  [GPU1] Ep6: R=3.80, Steps=30, Time=111.13s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (3, 6) towards (2, 5).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 5) towards (5, 3).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
  [GPU2] Ep7: R=6.20, Steps=30, Time=107.64s
  [GPU3] Ep7: R=2.40, Steps=30, Time=107.77s
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
  [GPU0] Ep7: R=4.20, Steps=30, Time=109.68s
Moving from (3, 5) towards (3, 7).
Moving from (6, 3) towards (4, 2).
Moving from (6, 2) towards (4, 2).
Moving from (5, 3) towards (4, 2).
  [GPU1] Ep7: R=5.80, Steps=30, Time=111.55s
  ‚úì Collected 32 episodes ([8, 8, 8, 8] per GPU)
  üìù Logged episode 3 trajectory
  Reward: 3.89¬±1.24 [2.40, 7.00], Steps: 30.0
  Low-level actions (GPU0 sample): up:16% | down:22% | left:22% | right:20% | clean:16% | eat:0% | stay:0%

  Inner Optimization (4 epochs, minibatch=8):
Training step failed: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.99 GiB is free. Including non-PyTorch memory, this process has 77.25 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1243, in train
    loss, clip_fraction, n_samples = self.compute_loss_on_samples(micro_batch, model_device)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1012, in compute_loss_on_samples
    new_log_probs = self.compute_batch_sequence_log_prob(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 600, in compute_batch_sequence_log_prob
    logits = forward_pass()
             ^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 596, in forward_pass
    outputs = model(input_ids=batch_input_ids, attention_mask=attention_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 786, in convert_to_fp32
    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 118, in recursively_apply
    {
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 119, in <dictcomp>
    k: recursively_apply(
       ^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 778, in _convert_to_fp32
    return tensor.float()
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.99 GiB is free. Including non-PyTorch memory, this process has 77.25 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[Group 19] Episodes 608-639 (expected)
Updating old model with current weights...
  ‚úì Old model updated
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (3, 5) towards (3, 7).
Moving from (3, 5) towards (5, 5).
Moving from (6, 3) towards (4, 2).
Moving from (6, 2) towards (4, 2).
Moving from (8, 5) towards (5, 5).
Moving from (8, 6) towards (5, 6).
  [GPU3] Ep0: R=3.80, Steps=30, Time=107.94s
Moving from (5, 3) towards (4, 2).
  [GPU2] Ep0: R=5.80, Steps=30, Time=108.51s
  [GPU0] Ep0: R=2.40, Steps=30, Time=110.24s
  [GPU1] Ep0: R=6.00, Steps=30, Time=111.51s
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (3, 6) towards (2, 5).
Moving from (4, 6) towards (2, 5).
  [GPU3] Ep1: R=3.80, Steps=30, Time=106.94s
  [GPU2] Ep1: R=4.00, Steps=30, Time=108.03s
  [GPU0] Ep1: R=3.80, Steps=30, Time=108.51s
  [GPU1] Ep1: R=3.80, Steps=30, Time=111.01s
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (3, 6) towards (2, 5).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (6, 3) towards (9, 2).
Moving from (6, 2) towards (9, 2).
Moving from (1, 5) towards (3, 7).
Moving from (7, 2) towards (9, 2).
Moving from (2, 5) towards (3, 7).
  [GPU3] Ep2: R=6.00, Steps=30, Time=106.06s
  [GPU2] Ep2: R=3.80, Steps=30, Time=107.55s
  [GPU0] Ep2: R=3.00, Steps=30, Time=108.79s
  [GPU1] Ep2: R=3.80, Steps=30, Time=111.05s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU3] Ep3: R=3.60, Steps=30, Time=106.04s
  [GPU2] Ep3: R=3.80, Steps=30, Time=107.82s
  [GPU0] Ep3: R=3.80, Steps=30, Time=108.75s
  [GPU1] Ep3: R=4.00, Steps=30, Time=110.85s
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (3, 6) towards (2, 5).
Moving from (3, 5) towards (5, 5).
Moving from (8, 5) towards (5, 5).
Moving from (8, 6) towards (5, 6).
  [GPU3] Ep4: R=6.00, Steps=30, Time=106.06s
  [GPU2] Ep4: R=3.00, Steps=30, Time=107.89s
  [GPU0] Ep4: R=3.00, Steps=30, Time=107.31s
  [GPU1] Ep4: R=3.80, Steps=30, Time=110.95s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (3, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
  [GPU3] Ep5: R=4.00, Steps=30, Time=106.26s
Moving from (8, 6) towards (12, 8).
Moving from (5, 8) towards (1, 8).
  [GPU2] Ep5: R=3.00, Steps=30, Time=107.96s
  [GPU0] Ep5: R=3.00, Steps=30, Time=109.37s
  [GPU1] Ep5: R=4.20, Steps=30, Time=110.95s
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
  [GPU3] Ep6: R=2.40, Steps=30, Time=106.29s
  [GPU2] Ep6: R=2.40, Steps=30, Time=107.80s
Moving from (6, 3) towards (9, 2).
Moving from (6, 2) towards (9, 2).
Moving from (1, 5) towards (3, 7).
Moving from (7, 2) towards (9, 2).
Moving from (2, 5) towards (3, 7).
  [GPU0] Ep6: R=6.00, Steps=30, Time=108.69s
  [GPU1] Ep6: R=3.80, Steps=30, Time=110.78s
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
  [GPU3] Ep7: R=4.00, Steps=30, Time=106.34s
Moving from (2, 8) towards (2, 5).
  [GPU2] Ep7: R=4.00, Steps=30, Time=107.62s
Moving from (2, 7) towards (2, 5).
  [GPU0] Ep7: R=4.00, Steps=30, Time=108.82s
Moving from (3, 5) towards (5, 5).
Moving from (8, 5) towards (5, 5).
Moving from (8, 6) towards (5, 6).
  [GPU1] Ep7: R=6.00, Steps=30, Time=110.56s
  ‚úì Collected 32 episodes ([8, 8, 8, 8] per GPU)
  üìù Logged episode 4 trajectory
  Reward: 3.99¬±1.09 [2.40, 6.00], Steps: 30.0
  Low-level actions (GPU0 sample): up:18% | down:23% | left:21% | right:19% | clean:15% | eat:0% | stay:0%

  Inner Optimization (4 epochs, minibatch=8):
Training step failed: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.91 GiB is free. Including non-PyTorch memory, this process has 77.33 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1243, in train
    loss, clip_fraction, n_samples = self.compute_loss_on_samples(micro_batch, model_device)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1012, in compute_loss_on_samples
    new_log_probs = self.compute_batch_sequence_log_prob(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 600, in compute_batch_sequence_log_prob
    logits = forward_pass()
             ^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 596, in forward_pass
    outputs = model(input_ids=batch_input_ids, attention_mask=attention_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 786, in convert_to_fp32
    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 118, in recursively_apply
    {
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 119, in <dictcomp>
    k: recursively_apply(
       ^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 778, in _convert_to_fp32
    return tensor.float()
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.91 GiB is free. Including non-PyTorch memory, this process has 77.33 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[Group 20] Episodes 640-671 (expected)
Updating old model with current weights...
  ‚úì Old model updated

  Sample generation (compound):
    Obs: You are at (5, 3). You see dirt at (7, 2) and dirt at (7, 4).
    Response (thinking+JSON): '{
    "action": "clean_at",
    "agent_id": 1,
    "args": {
        "coord_x": 7,
        "coord_y": 2
    }
}'
    ‚Üí Low-level action: right
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (3, 8) towards (1, 8).
Moving from (3, 6) towards (2, 5).
Moving from (3, 6) towards (2, 5).
Moving from (2, 5) towards (1, 3).
Moving from (0, 4) towards (1, 3).
  [GPU3] Ep0: R=7.20, Steps=30, Time=107.00s
  [GPU2] Ep0: R=3.80, Steps=30, Time=107.82s
[CUDA Memory - After episode rollout] Allocated: 30.43GB, Reserved: 76.71GB, Peak: 74.33GB
  [GPU1] Ep0: R=3.80, Steps=30, Time=111.35s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (8, 7) towards (12, 8).
Moving from (9, 7) towards (12, 8).
Moving from (10, 7) towards (12, 8).
  [GPU3] Ep1: R=3.00, Steps=30, Time=105.34s
  [GPU2] Ep1: R=4.20, Steps=30, Time=107.05s
  [GPU0] Ep1: R=3.00, Steps=30, Time=109.03s
  [GPU1] Ep1: R=2.80, Steps=30, Time=110.93s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (9, 5) towards (12, 8).
Moving from (5, 8) towards (1, 8).
  [GPU3] Ep2: R=3.80, Steps=30, Time=106.31s
  [GPU2] Ep2: R=3.00, Steps=30, Time=107.86s
  [GPU0] Ep2: R=4.00, Steps=30, Time=108.11s
  [GPU1] Ep2: R=4.00, Steps=30, Time=111.05s
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU3] Ep3: R=3.00, Steps=30, Time=106.27s
  [GPU2] Ep3: R=2.40, Steps=30, Time=107.54s
  [GPU0] Ep3: R=3.00, Steps=30, Time=108.18s
  [GPU1] Ep3: R=4.00, Steps=30, Time=111.29s
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (8, 6) towards (12, 8).
  [GPU3] Ep4: R=4.00, Steps=30, Time=106.22s
  [GPU2] Ep4: R=3.00, Steps=30, Time=107.38s
  [GPU0] Ep4: R=3.00, Steps=30, Time=107.29s
  [GPU1] Ep4: R=4.20, Steps=30, Time=110.89s
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (3, 6) towards (2, 5).
Moving from (2, 8) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (2, 7) towards (2, 5).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (3, 6) towards (2, 5).
Moving from (8, 5) towards (9, 6).
Moving from (7, 2) towards (4, 2).
Moving from (1, 5) towards (3, 7).
Moving from (2, 5) towards (3, 7).
  [GPU3] Ep5: R=7.00, Steps=30, Time=106.65s
  [GPU2] Ep5: R=3.80, Steps=30, Time=108.19s
  [GPU0] Ep5: R=2.80, Steps=30, Time=107.59s
  [GPU1] Ep5: R=3.80, Steps=30, Time=111.01s
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (4, 6) towards (2, 5).
Moving from (2, 7) towards (2, 5).
  [GPU3] Ep6: R=2.80, Steps=30, Time=106.15s
Moving from (3, 5) towards (5, 5).
Moving from (8, 5) towards (5, 5).
Moving from (8, 6) towards (5, 6).
  [GPU2] Ep6: R=4.00, Steps=30, Time=107.32s
  [GPU0] Ep6: R=6.00, Steps=30, Time=107.97s
  [GPU1] Ep6: R=4.00, Steps=30, Time=110.91s
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 3) towards (9, 2).
Moving from (6, 2) towards (9, 2).
Moving from (1, 5) towards (3, 7).
Moving from (7, 2) towards (9, 2).
Moving from (2, 5) towards (3, 7).
  [GPU3] Ep7: R=6.00, Steps=30, Time=106.28s
Moving from (8, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
  [GPU2] Ep7: R=4.00, Steps=30, Time=107.50s
Moving from (2, 8) towards (2, 5).
  [GPU0] Ep7: R=3.20, Steps=30, Time=108.76s
Moving from (2, 7) towards (2, 5).
Moving from (6, 3) towards (9, 2).
Moving from (6, 2) towards (9, 2).
Moving from (1, 5) towards (3, 7).
Moving from (7, 2) towards (9, 2).
Moving from (2, 5) towards (3, 7).
  [GPU1] Ep7: R=6.00, Steps=30, Time=111.07s
  ‚úì Collected 32 episodes ([8, 8, 8, 8] per GPU)
  üìù Logged episode 4 trajectory
  Reward: 3.92¬±1.23 [2.40, 7.20], Steps: 30.0
  Low-level actions (GPU0 sample): up:16% | down:20% | left:23% | right:21% | clean:16% | eat:0% | stay:0%

  Inner Optimization (4 epochs, minibatch=8):
Training step failed: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.96 GiB is free. Including non-PyTorch memory, this process has 77.28 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1243, in train
    loss, clip_fraction, n_samples = self.compute_loss_on_samples(micro_batch, model_device)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1012, in compute_loss_on_samples
    new_log_probs = self.compute_batch_sequence_log_prob(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 600, in compute_batch_sequence_log_prob
    logits = forward_pass()
             ^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 596, in forward_pass
    outputs = model(input_ids=batch_input_ids, attention_mask=attention_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 786, in convert_to_fp32
    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 118, in recursively_apply
    {
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 119, in <dictcomp>
    k: recursively_apply(
       ^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 778, in _convert_to_fp32
    return tensor.float()
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.96 GiB is free. Including non-PyTorch memory, this process has 77.28 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[Group 21] Episodes 672-703 (expected)
Updating old model with current weights...
  ‚úì Old model updated
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU3] Ep0: R=3.80, Steps=30, Time=106.78s
  [GPU2] Ep0: R=3.00, Steps=30, Time=108.60s
  [GPU0] Ep0: R=2.80, Steps=30, Time=108.66s
  [GPU1] Ep0: R=2.80, Steps=30, Time=111.91s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
  [GPU3] Ep1: R=4.20, Steps=30, Time=106.92s
  [GPU2] Ep1: R=4.00, Steps=30, Time=107.50s
  [GPU0] Ep1: R=2.80, Steps=30, Time=108.58s
  [GPU1] Ep1: R=3.00, Steps=30, Time=110.19s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU3] Ep2: R=4.00, Steps=30, Time=106.85s
  [GPU2] Ep2: R=2.80, Steps=30, Time=106.64s
  [GPU0] Ep2: R=3.60, Steps=30, Time=108.21s
  [GPU1] Ep2: R=4.00, Steps=30, Time=111.21s
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (3, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
  [GPU3] Ep3: R=3.00, Steps=30, Time=106.53s
  [GPU2] Ep3: R=3.80, Steps=30, Time=107.32s
  [GPU0] Ep3: R=4.00, Steps=30, Time=108.50s
  [GPU1] Ep3: R=4.20, Steps=30, Time=111.42s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU3] Ep4: R=4.00, Steps=30, Time=106.48s
  [GPU2] Ep4: R=2.80, Steps=30, Time=107.32s
  [GPU0] Ep4: R=3.00, Steps=30, Time=107.10s
  [GPU1] Ep4: R=3.80, Steps=30, Time=110.80s
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
  [GPU3] Ep5: R=4.20, Steps=30, Time=106.42s
Moving from (2, 8) towards (2, 5).
  [GPU2] Ep5: R=3.00, Steps=30, Time=105.83s
Moving from (2, 7) towards (2, 5).
  [GPU0] Ep5: R=2.80, Steps=30, Time=108.86s
Moving from (3, 5) towards (5, 5).
Moving from (8, 5) towards (5, 5).
Moving from (8, 6) towards (5, 6).
  [GPU1] Ep5: R=6.00, Steps=30, Time=110.89s
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
  [GPU3] Ep6: R=2.40, Steps=30, Time=106.52s
Moving from (6, 8) towards (1, 8).
  [GPU2] Ep6: R=4.00, Steps=30, Time=107.13s
Moving from (5, 8) towards (1, 8).
  [GPU0] Ep6: R=3.00, Steps=30, Time=109.17s
  [GPU1] Ep6: R=3.00, Steps=30, Time=111.24s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
  [GPU3] Ep7: R=2.80, Steps=30, Time=106.37s
  [GPU2] Ep7: R=3.00, Steps=30, Time=107.21s
  [GPU0] Ep7: R=3.00, Steps=30, Time=108.94s
  [GPU1] Ep7: R=4.00, Steps=30, Time=112.27s
  ‚úì Collected 32 episodes ([8, 8, 8, 8] per GPU)
  üìù Logged episode 7 trajectory
  Reward: 3.46¬±0.73 [2.40, 6.00], Steps: 30.0
  Low-level actions (GPU0 sample): up:18% | down:22% | left:21% | right:20% | clean:15% | eat:0% | stay:0%

  Inner Optimization (4 epochs, minibatch=8):
Training step failed: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 2.33 GiB is free. Including non-PyTorch memory, this process has 76.91 GiB memory in use. Of the allocated memory 74.21 GiB is allocated by PyTorch, and 1.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1243, in train
    loss, clip_fraction, n_samples = self.compute_loss_on_samples(micro_batch, model_device)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1012, in compute_loss_on_samples
    new_log_probs = self.compute_batch_sequence_log_prob(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 600, in compute_batch_sequence_log_prob
    logits = forward_pass()
             ^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 596, in forward_pass
    outputs = model(input_ids=batch_input_ids, attention_mask=attention_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 786, in convert_to_fp32
    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 118, in recursively_apply
    {
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 119, in <dictcomp>
    k: recursively_apply(
       ^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 778, in _convert_to_fp32
    return tensor.float()
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 2.33 GiB is free. Including non-PyTorch memory, this process has 76.91 GiB memory in use. Of the allocated memory 74.21 GiB is allocated by PyTorch, and 1.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[Group 22] Episodes 704-735 (expected)
Updating old model with current weights...
  ‚úì Old model updated
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (9, 4) towards (12, 8).
Moving from (5, 8) towards (1, 8).
  [GPU3] Ep0: R=3.00, Steps=30, Time=107.18s
  [GPU2] Ep0: R=3.00, Steps=30, Time=108.00s
  [GPU0] Ep0: R=4.00, Steps=30, Time=109.36s
  [GPU1] Ep0: R=2.80, Steps=30, Time=112.93s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (8, 7) towards (12, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
Moving from (7, 3) towards (5, 2).
Moving from (6, 3) towards (5, 2).
Moving from (3, 5) towards (5, 5).
  [GPU3] Ep1: R=2.80, Steps=30, Time=106.45s
Moving from (8, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
  [GPU2] Ep1: R=3.20, Steps=30, Time=106.97s
  [GPU0] Ep1: R=7.00, Steps=30, Time=109.03s
  [GPU1] Ep1: R=3.20, Steps=30, Time=112.43s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (4, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (3, 6) towards (2, 5).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
  [GPU3] Ep2: R=4.00, Steps=30, Time=106.05s
  [GPU2] Ep2: R=3.80, Steps=30, Time=106.37s
  [GPU0] Ep2: R=4.20, Steps=30, Time=109.42s
Moving from (3, 5) towards (3, 7).
Moving from (6, 3) towards (4, 2).
Moving from (6, 2) towards (4, 2).
Moving from (5, 3) towards (4, 2).
  [GPU1] Ep2: R=5.80, Steps=30, Time=110.87s
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
  [GPU3] Ep3: R=3.80, Steps=30, Time=105.89s
  [GPU2] Ep3: R=2.80, Steps=30, Time=106.39s
Moving from (8, 5) towards (9, 6).
  [GPU0] Ep3: R=2.80, Steps=30, Time=109.56s
Moving from (7, 2) towards (4, 2).
Moving from (1, 5) towards (3, 7).
Moving from (2, 5) towards (3, 7).
  [GPU1] Ep3: R=7.00, Steps=30, Time=111.59s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (2, 7) towards (2, 5).
Moving from (3, 5) towards (5, 5).
Moving from (8, 5) towards (5, 5).
Moving from (8, 6) towards (5, 6).
  [GPU3] Ep4: R=3.00, Steps=30, Time=106.23s
  [GPU2] Ep4: R=6.00, Steps=30, Time=107.11s
  [GPU0] Ep4: R=4.00, Steps=30, Time=109.49s
  [GPU1] Ep4: R=2.40, Steps=30, Time=112.11s
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (8, 7) towards (12, 8).
Moving from (6, 5) towards (2, 5).
  [GPU3] Ep5: R=3.00, Steps=30, Time=106.01s
  [GPU2] Ep5: R=3.20, Steps=30, Time=107.77s
  [GPU0] Ep5: R=3.80, Steps=30, Time=109.27s
  [GPU1] Ep5: R=3.00, Steps=30, Time=110.72s
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (5, 7) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (3, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
Moving from (2, 8) towards (2, 5).
Moving from (3, 8) towards (1, 8).
Moving from (2, 7) towards (2, 5).
  [GPU3] Ep6: R=4.00, Steps=30, Time=106.15s
  [GPU2] Ep6: R=4.20, Steps=30, Time=107.52s
Moving from (2, 8) towards (2, 5).
Moving from (3, 5) towards (3, 7).
Moving from (2, 7) towards (2, 5).
Moving from (6, 3) towards (4, 2).
Moving from (6, 2) towards (4, 2).
Moving from (5, 3) towards (4, 2).
  [GPU0] Ep6: R=5.80, Steps=30, Time=109.15s
Moving from (8, 5) towards (5, 5).
Moving from (8, 6) towards (5, 6).
  [GPU1] Ep6: R=6.00, Steps=30, Time=111.53s
Moving from (6, 8) towards (1, 8).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (6, 5) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 6) towards (2, 5).
Moving from (5, 8) towards (1, 8).
Moving from (4, 6) towards (2, 5).
Moving from (6, 8) towards (1, 8).
Moving from (7, 7) towards (12, 8).
Moving from (5, 8) towards (1, 8).
Moving from (4, 8) towards (1, 8).
  [GPU3] Ep7: R=3.00, Steps=30, Time=106.49s
Moving from (3, 8) towards (1, 8).
  [GPU2] Ep7: R=2.40, Steps=30, Time=108.08s
Moving from (2, 8) towards (2, 5).
Moving from (2, 7) towards (2, 5).
  [GPU0] Ep7: R=4.00, Steps=30, Time=108.81s
Moving from (3, 5) towards (5, 5).
Moving from (8, 5) towards (5, 5).
Moving from (8, 6) towards (5, 6).
  [GPU1] Ep7: R=6.00, Steps=30, Time=111.82s
  ‚úì Collected 32 episodes ([8, 8, 8, 8] per GPU)
  üìù Logged episode 6 trajectory
  Reward: 3.97¬±1.34 [2.40, 7.00], Steps: 30.0
  Low-level actions (GPU0 sample): up:17% | down:21% | left:23% | right:19% | clean:16% | eat:1% | stay:0%

  Inner Optimization (4 epochs, minibatch=8):
Training step failed: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 77.27 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1243, in train
    loss, clip_fraction, n_samples = self.compute_loss_on_samples(micro_batch, model_device)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 1012, in compute_loss_on_samples
    new_log_probs = self.compute_batch_sequence_log_prob(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 600, in compute_batch_sequence_log_prob
    logits = forward_pass()
             ^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/Research/LLM_MARL/paper/archive/grpo_text_compound.py", line 596, in forward_pass
    outputs = model(input_ids=batch_input_ids, attention_mask=attention_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 786, in convert_to_fp32
    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 118, in recursively_apply
    {
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 119, in <dictcomp>
    k: recursively_apply(
       ^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 126, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/utils/operations.py", line 778, in _convert_to_fp32
    return tensor.float()
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 77.27 GiB memory in use. Of the allocated memory 74.20 GiB is allocated by PyTorch, and 1.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[Group 23] Episodes 736-767 (expected)
Updating old model with current weights...
  ‚úì Old model updated
W0224 11:48:56.486000 3582027 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3582078 closing signal SIGTERM
W0224 11:48:56.488000 3582027 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3582080 closing signal SIGTERM
W0224 11:48:56.488000 3582027 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3582081 closing signal SIGTERM
E0224 11:48:57.568000 3582027 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -11) local_rank: 1 (pid: 3582079) of binary: /users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/bin/python
Traceback (most recent call last):
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1185, in launch_command
    multi_gpu_launcher(args)
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/accelerate/commands/launch.py", line 810, in multi_gpu_launcher
    distrib_run.run(args)
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/users/PAS2056/mypeter8219/miniconda3/envs/py311LLM/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=========================================================
grpo_text_compound.py FAILED
---------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
---------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-02-24_11:48:56
  host      : a0018.ten.osc.edu
  rank      : 1 (local_rank: 1)
  exitcode  : -11 (pid: 3582079)
  error_file: <N/A>
  traceback : Signal 11 (SIGSEGV) received by PID 3582079
=========================================================
